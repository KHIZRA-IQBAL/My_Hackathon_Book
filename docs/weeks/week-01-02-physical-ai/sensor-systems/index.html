<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-weeks/week-01-02-physical-ai/sensor-systems" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Sensor Systems for Humanoid Robots | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://khizra-iqbal.github.io/My_Hackathon_Book/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://khizra-iqbal.github.io/My_Hackathon_Book/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://khizra-iqbal.github.io/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Sensor Systems for Humanoid Robots | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Understanding LIDAR, cameras, IMUs, force sensors, and multi-sensor fusion"><meta data-rh="true" property="og:description" content="Understanding LIDAR, cameras, IMUs, force sensors, and multi-sensor fusion"><link data-rh="true" rel="icon" href="/My_Hackathon_Book/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://khizra-iqbal.github.io/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems"><link data-rh="true" rel="alternate" href="https://khizra-iqbal.github.io/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems" hreflang="en"><link data-rh="true" rel="alternate" href="https://khizra-iqbal.github.io/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Sensor Systems for Humanoid Robots","item":"https://khizra-iqbal.github.io/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems"}]}</script><link rel="alternate" type="application/rss+xml" href="/My_Hackathon_Book/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/My_Hackathon_Book/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Atom Feed"><link rel="stylesheet" href="/My_Hackathon_Book/assets/css/styles.7c88f103.css">
<script src="/My_Hackathon_Book/assets/js/runtime~main.dfffce87.js" defer="defer"></script>
<script src="/My_Hackathon_Book/assets/js/main.14bb32f2.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/My_Hackathon_Book/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/My_Hackathon_Book/"><div class="navbar__logo"><img src="/My_Hackathon_Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/My_Hackathon_Book/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/My_Hackathon_Book/docs/">Course Content</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/KHIZRA-IQBAL/My_Hackathon_Book" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/My_Hackathon_Book/docs/"><span title="Physical AI &amp; Humanoid Robotics" class="linkLabel_WmDU">Physical AI &amp; Humanoid Robotics</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/intro"><span title="weeks" class="categoryLinkLabel_W154">weeks</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/intro"><span title="week-01-02-physical-ai" class="categoryLinkLabel_W154">week-01-02-physical-ai</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/intro"><span title="Week 1-2: Physical AI Foundations" class="linkLabel_WmDU">Week 1-2: Physical AI Foundations</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/what-is-physical-ai"><span title="What is Physical AI?" class="linkLabel_WmDU">What is Physical AI?</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/embodied-intelligence"><span title="Embodied Intelligence" class="linkLabel_WmDU">Embodied Intelligence</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/robotics-landscape"><span title="Humanoid Robotics Landscape" class="linkLabel_WmDU">Humanoid Robotics Landscape</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems"><span title="Sensor Systems for Humanoid Robots" class="linkLabel_WmDU">Sensor Systems for Humanoid Robots</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/assessment"><span title="Assessment" class="linkLabel_WmDU">Assessment</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-03-05-ros2/intro"><span title="week-03-05-ros2" class="categoryLinkLabel_W154">week-03-05-ros2</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-06-07-gazebo/intro"><span title="week-06-07-gazebo" class="categoryLinkLabel_W154">week-06-07-gazebo</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-08-10-nvidia-isaac/intro"><span title="week-08-10-nvidia-isaac" class="categoryLinkLabel_W154">week-08-10-nvidia-isaac</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-11-12-humanoid/intro"><span title="week-11-12-humanoid" class="categoryLinkLabel_W154">week-11-12-humanoid</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/My_Hackathon_Book/docs/weeks/week-13-conversational/intro"><span title="week-13-conversational" class="categoryLinkLabel_W154">week-13-conversational</span></a></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/My_Hackathon_Book/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">weeks</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">week-01-02-physical-ai</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Sensor Systems for Humanoid Robots</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Sensor Systems for Humanoid Robots</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>Humanoid robots must perceive their environment to navigate, manipulate objects, and interact with humans. Unlike digital AI systems that receive clean, structured inputs, physical robots rely on <strong>sensors</strong>devices that convert physical quantities (light, distance, force, acceleration) into electrical signals.</p>
<p>This page explores four primary sensor modalities used in humanoid robotics:</p>
<ol>
<li class=""><strong>LIDAR</strong>: 3D point cloud generation for spatial mapping</li>
<li class=""><strong>Cameras</strong>: RGB, depth, and stereo vision for object recognition</li>
<li class=""><strong>IMUs</strong>: Accelerometers and gyroscopes for orientation tracking</li>
<li class=""><strong>Force/Torque Sensors</strong>: Contact detection and grip control</li>
</ol>
<p>You will learn the operating principles, specifications, limitations, and typical use cases for each sensor type. Interactive code examples let you visualize sensor data and understand how multi-sensor fusion improves robustness.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-data-flow-in-robot-architecture">Sensor Data Flow in Robot Architecture<a href="#sensor-data-flow-in-robot-architecture" class="hash-link" aria-label="Direct link to Sensor Data Flow in Robot Architecture" title="Direct link to Sensor Data Flow in Robot Architecture" translate="no">​</a></h2>
<p>Before diving into individual sensors, consider how sensor data flows through a robot system:</p>
<div class="language-mermaid codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-mermaid codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">graph TB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph Sensors[&quot;Sensor Layer&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        L[LIDAR&lt;br/&gt;Point Clouds]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        C[Cameras&lt;br/&gt;RGB/Depth]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        I[IMU&lt;br/&gt;Accel/Gyro]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        F[Force/Torque&lt;br/&gt;Contact]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph Processing[&quot;Processing Layer&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        LP[Point Cloud&lt;br/&gt;Processing]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        CV[Computer Vision&lt;br/&gt;Object Detection]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        OR[Orientation&lt;br/&gt;Tracking]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        CT[Contact&lt;br/&gt;Detection]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph Fusion[&quot;Fusion Layer&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        KF[Kalman Filter&lt;br/&gt;State Estimation]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        SLAM[SLAM&lt;br/&gt;Localization]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    subgraph Decision[&quot;Decision Layer&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        MP[Motion Planning]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        GC[Grasp Control]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    end</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    L --&gt; LP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    C --&gt; CV</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    I --&gt; OR</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    F --&gt; CT</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    LP --&gt; KF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    CV --&gt; KF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    OR --&gt; KF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    CT --&gt; KF</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    LP --&gt; SLAM</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    CV --&gt; SLAM</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    KF --&gt; MP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    SLAM --&gt; MP</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    KF --&gt; GC</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    CT --&gt; GC</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style L fill:#ffebcc,stroke:#ff9900</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style C fill:#ccebff,stroke:#0099ff</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style I fill:#ffcceb,stroke:#ff0099</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style F fill:#ccffeb,stroke:#00ff99</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style KF fill:#ffe6e6,stroke:#ff0000,stroke-width:3px</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style SLAM fill:#ffe6e6,stroke:#ff0000,stroke-width:3px</span><br></span></code></pre></div></div>
<p><strong>Figure 1</strong>: Sensor data flows from raw measurements through processing layers to fusion algorithms (Kalman Filter, SLAM) that combine multiple sensor inputs for robust state estimation. Motion planning and grasp control consume fused state estimates.</p>
<p><strong>Key Insight</strong>: No single sensor is sufficient. Robust perception requires <strong>multi-sensor fusion</strong> to overcome individual sensor limitations.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="lidar-3d-point-cloud-generation">LIDAR: 3D Point Cloud Generation<a href="#lidar-3d-point-cloud-generation" class="hash-link" aria-label="Direct link to LIDAR: 3D Point Cloud Generation" title="Direct link to LIDAR: 3D Point Cloud Generation" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="operating-principle">Operating Principle<a href="#operating-principle" class="hash-link" aria-label="Direct link to Operating Principle" title="Direct link to Operating Principle" translate="no">​</a></h3>
<p><strong>LIDAR</strong> (Light Detection and Ranging) measures distances by emitting laser pulses and timing their reflection:</p>
<ol>
<li class="">Laser emits a short pulse of light</li>
<li class="">Light reflects off objects in the environment</li>
<li class="">Sensor detects the reflected light</li>
<li class=""><strong>Time-of-flight</strong> calculation: distance = (speed of light � time) / 2</li>
</ol>
<p>By rotating the laser or using multiple laser channels, LIDAR generates a <strong>3D point cloud</strong>a set of (x, y, z) coordinates representing surfaces in the environment.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="velodyne-vlp-16-specifications">Velodyne VLP-16 Specifications<a href="#velodyne-vlp-16-specifications" class="hash-link" aria-label="Direct link to Velodyne VLP-16 Specifications" title="Direct link to Velodyne VLP-16 Specifications" translate="no">​</a></h3>
<p>The <a href="https://pdf.directindustry.com/pdf/velodynelidar/vlp-16-datasheets/182407-676097.html" target="_blank" rel="noopener noreferrer" class="">Velodyne VLP-16</a> is a popular LIDAR sensor used in robotics research and autonomous vehicles:</p>
<table><thead><tr><th>Specification</th><th>Value</th></tr></thead><tbody><tr><td><strong>Channels</strong></td><td>16</td></tr><tr><td><strong>Range</strong></td><td>100 m</td></tr><tr><td><strong>Precision</strong></td><td>�3 cm</td></tr><tr><td><strong>Horizontal FOV</strong></td><td>360�</td></tr><tr><td><strong>Vertical FOV</strong></td><td>30� (�15�)</td></tr><tr><td><strong>Horizontal Resolution</strong></td><td>0.2� at 10 Hz</td></tr><tr><td><strong>Vertical Resolution</strong></td><td>2�</td></tr><tr><td><strong>Data Rate</strong></td><td>~300,000 points/second</td></tr><tr><td><strong>Power</strong></td><td>8 W</td></tr><tr><td><strong>Weight</strong></td><td>830 g</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="when-to-use-lidar">When to Use LIDAR<a href="#when-to-use-lidar" class="hash-link" aria-label="Direct link to When to Use LIDAR" title="Direct link to When to Use LIDAR" translate="no">​</a></h3>
<p><strong>Strengths:</strong></p>
<ul>
<li class="">Accurate 3D spatial mapping (�3 cm precision)</li>
<li class="">Works in darkness (active sensor, provides its own illumination)</li>
<li class="">Long range (up to 100 m)</li>
<li class="">Direct distance measurements (no stereo correspondence required)</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li class="">No color information (only geometric data)</li>
<li class="">Expensive ($4,000-$8,000 for VLP-16)</li>
<li class="">Reflective or transparent surfaces cause errors</li>
<li class="">Lower resolution than cameras (thousands of points vs. millions of pixels)</li>
</ul>
<p><strong>Typical Use Cases:</strong></p>
<ul>
<li class="">Navigation and obstacle avoidance</li>
<li class="">3D mapping (SLAM - Simultaneous Localization and Mapping)</li>
<li class="">Terrain analysis for legged robots</li>
<li class="">Object detection and tracking</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="interactive-example-lidar-point-cloud-visualization">Interactive Example: LIDAR Point Cloud Visualization<a href="#interactive-example-lidar-point-cloud-visualization" class="hash-link" aria-label="Direct link to Interactive Example: LIDAR Point Cloud Visualization" title="Direct link to Interactive Example: LIDAR Point Cloud Visualization" translate="no">​</a></h3>
<p><a href="https://colab.research.google.com/github/KHIZRA-IQBAL/My_Hackathon_Book/blob/main/book/colab/week-01-02/lidar-point-cloud.ipynb" target="_blank" rel="noopener noreferrer" class=""><img decoding="async" loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" class="img_ev3q"></a></p>
<p>This notebook simulates a LIDAR scan of a room with walls and obstacles, then visualizes the resulting 3D point cloud.</p>
<p><strong>What to observe:</strong></p>
<ul>
<li class="">Sparse 3D representation (thousands of points, not millions)</li>
<li class="">Uniform sampling in angular space (not Cartesian space)</li>
<li class="">How occlusions create &quot;shadows&quot; in the point cloud</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="camera-systems-rgb-depth-and-stereo-vision">Camera Systems: RGB, Depth, and Stereo Vision<a href="#camera-systems-rgb-depth-and-stereo-vision" class="hash-link" aria-label="Direct link to Camera Systems: RGB, Depth, and Stereo Vision" title="Direct link to Camera Systems: RGB, Depth, and Stereo Vision" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="types-of-cameras">Types of Cameras<a href="#types-of-cameras" class="hash-link" aria-label="Direct link to Types of Cameras" title="Direct link to Types of Cameras" translate="no">​</a></h3>
<ol>
<li class=""><strong>RGB Cameras</strong>: Capture color images (standard cameras)</li>
<li class=""><strong>Depth Cameras</strong>: Measure distance to each pixel</li>
<li class=""><strong>Stereo Cameras</strong>: Use two RGB cameras to compute depth through triangulation</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="intel-realsense-d455-depth-camera">Intel RealSense D455 Depth Camera<a href="#intel-realsense-d455-depth-camera" class="hash-link" aria-label="Direct link to Intel RealSense D455 Depth Camera" title="Direct link to Intel RealSense D455 Depth Camera" translate="no">​</a></h3>
<p>The <a href="https://www.intelrealsense.com/depth-camera-d455/" target="_blank" rel="noopener noreferrer" class="">Intel RealSense D455</a> uses <strong>stereo vision</strong> to compute depth:</p>
<table><thead><tr><th>Specification</th><th>Value</th></tr></thead><tbody><tr><td><strong>Depth Technology</strong></td><td>Stereo vision</td></tr><tr><td><strong>Range</strong></td><td>0.6 m to 6 m</td></tr><tr><td><strong>Depth Resolution</strong></td><td>1280�720 at up to 90 FPS</td></tr><tr><td><strong>Depth FOV</strong></td><td>86� � 57�</td></tr><tr><td><strong>RGB Sensor</strong></td><td>Yes (global shutter)</td></tr><tr><td><strong>IMU</strong></td><td>Yes</td></tr><tr><td><strong>Baseline</strong></td><td>95 mm (distance between stereo cameras)</td></tr><tr><td><strong>Depth Error</strong></td><td>&lt; 2% at 4 m</td></tr><tr><td><strong>Interface</strong></td><td>USB 3.1</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="when-to-use-each-camera-type">When to Use Each Camera Type<a href="#when-to-use-each-camera-type" class="hash-link" aria-label="Direct link to When to Use Each Camera Type" title="Direct link to When to Use Each Camera Type" translate="no">​</a></h3>
<p><strong>RGB Cameras:</strong></p>
<ul>
<li class="">Object recognition (colors, textures, text)</li>
<li class="">Semantic segmentation (classify pixels: person, chair, floor)</li>
<li class="">Visual servoing (tracking targets for manipulation)</li>
<li class="">Inexpensive and high resolution</li>
</ul>
<p><strong>Depth Cameras:</strong></p>
<ul>
<li class="">Obstacle avoidance (know distances to objects)</li>
<li class="">Grasp pose estimation (measure object dimensions)</li>
<li class="">3D reconstruction of scenes</li>
<li class="">Works in varied lighting (active infrared illumination)</li>
</ul>
<p><strong>Stereo Cameras:</strong></p>
<ul>
<li class="">Depth estimation without active illumination</li>
<li class="">Outdoor use (where structured light may fail)</li>
<li class="">Combines RGB and depth in single system</li>
</ul>
<p><strong>Trade-offs:</strong></p>
<ul>
<li class=""><strong>RGB alone</strong>: No depth information</li>
<li class=""><strong>Depth alone</strong>: Limited range (typically &lt; 10 m), sensitive to lighting</li>
<li class=""><strong>Stereo</strong>: Requires texture for correspondence (fails on blank walls)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="interactive-example-camera-image-processing">Interactive Example: Camera Image Processing<a href="#interactive-example-camera-image-processing" class="hash-link" aria-label="Direct link to Interactive Example: Camera Image Processing" title="Direct link to Interactive Example: Camera Image Processing" translate="no">​</a></h3>
<p><a href="https://colab.research.google.com/github/KHIZRA-IQBAL/My_Hackathon_Book/blob/main/book/colab/week-01-02/camera-image-processing.ipynb" target="_blank" rel="noopener noreferrer" class=""><img decoding="async" loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" class="img_ev3q"></a></p>
<p>This notebook demonstrates basic image processing: loading an image, applying edge detection, and visualizing results.</p>
<p><strong>What to observe:</strong></p>
<ul>
<li class="">High resolution compared to LIDAR (megapixels vs. thousands of points)</li>
<li class="">Edge detection reveals object boundaries</li>
<li class="">Color information enables semantic understanding</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="inertial-measurement-units-imus">Inertial Measurement Units (IMUs)<a href="#inertial-measurement-units-imus" class="hash-link" aria-label="Direct link to Inertial Measurement Units (IMUs)" title="Direct link to Inertial Measurement Units (IMUs)" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="operating-principle-1">Operating Principle<a href="#operating-principle-1" class="hash-link" aria-label="Direct link to Operating Principle" title="Direct link to Operating Principle" translate="no">​</a></h3>
<p>An <strong>IMU</strong> (Inertial Measurement Unit) combines:</p>
<ul>
<li class=""><strong>Accelerometer</strong>: Measures linear acceleration (including gravity)</li>
<li class=""><strong>Gyroscope</strong>: Measures angular velocity (rotation rate)</li>
<li class="">Often includes <strong>magnetometer</strong> for absolute heading (not covered here)</li>
</ul>
<p>By integrating accelerometer and gyroscope data, robots estimate <strong>orientation</strong> (roll, pitch, yaw) and <strong>velocity</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="bosch-bmi088-imu-specifications">Bosch BMI088 IMU Specifications<a href="#bosch-bmi088-imu-specifications" class="hash-link" aria-label="Direct link to Bosch BMI088 IMU Specifications" title="Direct link to Bosch BMI088 IMU Specifications" translate="no">​</a></h3>
<p>The <a href="https://www.bosch-sensortec.com/products/motion-sensors/imus/bmi088/" target="_blank" rel="noopener noreferrer" class="">Bosch BMI088</a> is a high-performance IMU used in robotics:</p>
<table><thead><tr><th>Component</th><th>Range</th><th>Resolution</th><th>Noise</th></tr></thead><tbody><tr><td><strong>Accelerometer</strong></td><td>�3g to �24g</td><td>16-bit</td><td>175 �g/Hz</td></tr><tr><td><strong>Gyroscope</strong></td><td>�125�/s to �2000�/s</td><td>16-bit</td><td>0.014�/s/Hz</td></tr><tr><td><strong>Output Data Rate</strong></td><td>Up to 2 kHz</td><td></td><td></td></tr><tr><td><strong>Interface</strong></td><td>SPI, I2C</td><td></td><td></td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-drift-problem">The Drift Problem<a href="#the-drift-problem" class="hash-link" aria-label="Direct link to The Drift Problem" title="Direct link to The Drift Problem" translate="no">​</a></h3>
<p>IMUs estimate orientation by <strong>integrating</strong> angular velocity:</p>
<p>�(t) = �(0) + +�(t) dt</p>
<p>However, sensor noise causes <strong>drift</strong>: small errors accumulate over time. After a few minutes, estimated orientation may be off by 10-20 degrees without correction.</p>
<p><strong>Solution</strong>: Fuse IMU data with other sensors (cameras, GPS, magnetometer) to correct drift.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="when-to-use-imus">When to Use IMUs<a href="#when-to-use-imus" class="hash-link" aria-label="Direct link to When to Use IMUs" title="Direct link to When to Use IMUs" translate="no">​</a></h3>
<p><strong>Strengths:</strong></p>
<ul>
<li class="">High update rate (1-2 kHz)</li>
<li class="">Compact and inexpensive ($10-$100)</li>
<li class="">Essential for balance control in legged robots</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li class="">Drift over time (requires sensor fusion)</li>
<li class="">No absolute position (only orientation and acceleration)</li>
<li class="">Sensitive to vibrations and electromagnetic interference</li>
</ul>
<p><strong>Typical Use Cases:</strong></p>
<ul>
<li class="">Balance control (detect tilt and adjust motors to prevent falling)</li>
<li class="">Sensor fusion for state estimation</li>
<li class="">Dead reckoning (short-term position estimation)</li>
<li class="">Detecting impacts or collisions</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="interactive-example-imu-orientation-tracking">Interactive Example: IMU Orientation Tracking<a href="#interactive-example-imu-orientation-tracking" class="hash-link" aria-label="Direct link to Interactive Example: IMU Orientation Tracking" title="Direct link to Interactive Example: IMU Orientation Tracking" translate="no">​</a></h3>
<p><a href="https://colab.research.google.com/github/KHIZRA-IQBAL/My_Hackathon_Book/blob/main/book/colab/week-01-02/imu-orientation-tracking.ipynb" target="_blank" rel="noopener noreferrer" class=""><img decoding="async" loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" class="img_ev3q"></a></p>
<p>This notebook simulates an IMU measuring rotation, integrates angular velocity to estimate orientation, and demonstrates how noise causes drift over time.</p>
<p><strong>What to observe:</strong></p>
<ul>
<li class="">True orientation vs. estimated orientation diverge</li>
<li class="">Drift accumulates linearly with time</li>
<li class="">Need for sensor fusion to correct errors</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="forcetorque-sensors">Force/Torque Sensors<a href="#forcetorque-sensors" class="hash-link" aria-label="Direct link to Force/Torque Sensors" title="Direct link to Force/Torque Sensors" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="operating-principle-2">Operating Principle<a href="#operating-principle-2" class="hash-link" aria-label="Direct link to Operating Principle" title="Direct link to Operating Principle" translate="no">​</a></h3>
<p><strong>Force/torque sensors</strong> measure mechanical forces and moments applied to a robot&#x27;s end-effector (gripper, foot, wrist). They use <strong>strain gauges</strong>resistive elements that change resistance under deformation.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="ati-mini45-specifications">ATI Mini45 Specifications<a href="#ati-mini45-specifications" class="hash-link" aria-label="Direct link to ATI Mini45 Specifications" title="Direct link to ATI Mini45 Specifications" translate="no">​</a></h3>
<p>The <a href="https://www.ati-ia.com/products/ft/ft_models.aspx?id=Mini45" target="_blank" rel="noopener noreferrer" class="">ATI Mini45</a> is a compact 6-axis force/torque sensor:</p>
<table><thead><tr><th>Specification</th><th>Value</th></tr></thead><tbody><tr><td><strong>Force Range</strong></td><td>�290 N (z-axis), �580 N (x/y-axis)</td></tr><tr><td><strong>Torque Range</strong></td><td>�10 Nm</td></tr><tr><td><strong>Resolution</strong></td><td>1/50 N (force), 1/500 Nm (torque)</td></tr><tr><td><strong>Dimensions</strong></td><td>�45 mm � 15 mm</td></tr><tr><td><strong>Weight</strong></td><td>100 g</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="when-to-use-forcetorque-sensors">When to Use Force/Torque Sensors<a href="#when-to-use-forcetorque-sensors" class="hash-link" aria-label="Direct link to When to Use Force/Torque Sensors" title="Direct link to When to Use Force/Torque Sensors" translate="no">​</a></h3>
<p><strong>Strengths:</strong></p>
<ul>
<li class="">Detect contact (essential for safe manipulation)</li>
<li class="">Measure grip force (avoid crushing or dropping objects)</li>
<li class="">Enable compliant control (robot yields to external forces)</li>
<li class="">Force feedback for delicate assembly tasks</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li class="">Expensive ($2,000-$5,000)</li>
<li class="">Requires careful calibration</li>
<li class="">Limited to measuring forces at sensor location (not distributed sensing)</li>
</ul>
<p><strong>Typical Use Cases:</strong></p>
<ul>
<li class=""><strong>Grasp control</strong>: Adjust grip force based on object weight and fragility</li>
<li class=""><strong>Contact detection</strong>: Know when hand touches an object</li>
<li class=""><strong>Compliant manipulation</strong>: Allow robot to comply with external forces (polishing, wiping)</li>
<li class=""><strong>Safety</strong>: Detect collisions and stop motion</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="multi-sensor-fusion">Multi-Sensor Fusion<a href="#multi-sensor-fusion" class="hash-link" aria-label="Direct link to Multi-Sensor Fusion" title="Direct link to Multi-Sensor Fusion" translate="no">​</a></h2>
<p>No single sensor provides complete, reliable information. Robust robot perception requires <strong>sensor fusion</strong>combining data from multiple sensors to overcome individual limitations.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-fusion-is-necessary">Why Fusion is Necessary<a href="#why-fusion-is-necessary" class="hash-link" aria-label="Direct link to Why Fusion is Necessary" title="Direct link to Why Fusion is Necessary" translate="no">​</a></h3>
<table><thead><tr><th>Sensor</th><th>Strengths</th><th>Weaknesses</th><th>Fusion Partner</th></tr></thead><tbody><tr><td><strong>LIDAR</strong></td><td>Accurate 3D, long range</td><td>No color, expensive</td><td>Cameras (for semantics)</td></tr><tr><td><strong>RGB Camera</strong></td><td>High resolution, color</td><td>No depth</td><td>Depth camera or LIDAR</td></tr><tr><td><strong>Depth Camera</strong></td><td>Direct depth</td><td>Limited range, indoor</td><td>RGB camera or LIDAR</td></tr><tr><td><strong>IMU</strong></td><td>High rate, orientation</td><td>Drift</td><td>Cameras, GPS</td></tr><tr><td><strong>Force Sensor</strong></td><td>Contact detection</td><td>Local measurement</td><td>Vision (predict contact)</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="fusion-techniques">Fusion Techniques<a href="#fusion-techniques" class="hash-link" aria-label="Direct link to Fusion Techniques" title="Direct link to Fusion Techniques" translate="no">​</a></h3>
<ol>
<li class="">
<p><strong>Weighted Averaging</strong>: Combine sensor readings based on reliability</p>
<ul>
<li class="">Example: Average LIDAR and depth camera distance estimates, weighting LIDAR higher</li>
</ul>
</li>
<li class="">
<p><strong>Kalman Filtering</strong>: Optimal fusion assuming Gaussian noise</p>
<ul>
<li class="">Predict robot state using motion model</li>
<li class="">Update prediction using sensor measurements</li>
<li class="">Commonly used for IMU + camera fusion</li>
</ul>
</li>
<li class="">
<p><strong>SLAM (Simultaneous Localization and Mapping)</strong>: Fuse LIDAR/camera with odometry</p>
<ul>
<li class="">Build map of environment while estimating robot position</li>
<li class="">Used in autonomous navigation</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="interactive-example-multi-sensor-fusion">Interactive Example: Multi-Sensor Fusion<a href="#interactive-example-multi-sensor-fusion" class="hash-link" aria-label="Direct link to Interactive Example: Multi-Sensor Fusion" title="Direct link to Interactive Example: Multi-Sensor Fusion" translate="no">​</a></h3>
<p><a href="https://colab.research.google.com/github/KHIZRA-IQBAL/My_Hackathon_Book/blob/main/book/colab/week-01-02/multi-sensor-fusion.ipynb" target="_blank" rel="noopener noreferrer" class=""><img decoding="async" loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" class="img_ev3q"></a></p>
<p>This notebook simulates two noisy sensors measuring the same quantity and demonstrates how weighted averaging reduces uncertainty.</p>
<p><strong>What to observe:</strong></p>
<ul>
<li class="">Fused estimate has lower noise than either individual sensor</li>
<li class="">Weights depend on sensor reliability (lower noise � higher weight)</li>
<li class="">Fusion improves as more sensors are added</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="deep-dive-kalman-filter-basics">Deep Dive: Kalman Filter Basics<a href="#deep-dive-kalman-filter-basics" class="hash-link" aria-label="Direct link to Deep Dive: Kalman Filter Basics" title="Direct link to Deep Dive: Kalman Filter Basics" translate="no">​</a></h2>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Deep Dive: Kalman Filter for Sensor Fusion</div><div class="admonitionContent_BuS1"><p>The <strong>Kalman Filter</strong> is the gold standard for fusing sensor data under Gaussian noise assumptions. It operates in two steps:</p><h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-prediction-step-motion-model">1. Prediction Step (Motion Model)<a href="#1-prediction-step-motion-model" class="hash-link" aria-label="Direct link to 1. Prediction Step (Motion Model)" title="Direct link to 1. Prediction Step (Motion Model)" translate="no">​</a></h3><p>Predict the next state based on dynamics:</p><ul>
<li class="">x(t|t-1) = F�x(t-1) + B�u(t)</li>
<li class="">P(t|t-1) = F�P(t-1)�F^T + Q</li>
</ul><p>Where:</p><ul>
<li class="">x: State estimate (position, velocity, orientation)</li>
<li class="">F: State transition matrix (how state evolves)</li>
<li class="">B: Control input matrix</li>
<li class="">u: Control input (motor commands)</li>
<li class="">P: Covariance matrix (uncertainty)</li>
<li class="">Q: Process noise covariance</li>
</ul><h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-update-step-measurement">2. Update Step (Measurement)<a href="#2-update-step-measurement" class="hash-link" aria-label="Direct link to 2. Update Step (Measurement)" title="Direct link to 2. Update Step (Measurement)" translate="no">​</a></h3><p>Correct prediction using sensor measurement:</p><ul>
<li class="">K = P(t|t-1)�H^T�(H�P(t|t-1)�H^T + R)^-1  (Kalman Gain)</li>
<li class="">x(t|t) = x(t|t-1) + K�(z - H�x(t|t-1))</li>
<li class="">P(t|t) = (I - K�H)�P(t|t-1)</li>
</ul><p>Where:</p><ul>
<li class="">z: Sensor measurement</li>
<li class="">H: Measurement matrix (maps state to sensor reading)</li>
<li class="">R: Measurement noise covariance</li>
<li class="">K: Kalman gain (optimal weighting)</li>
</ul><p><strong>Key Insight</strong>: Kalman gain automatically balances prediction and measurement based on their relative uncertainties. If sensors are noisy (high R), trust the prediction more. If motion model is uncertain (high Q), trust measurements more.</p><p><strong>Further Reading:</strong></p><ul>
<li class="">Thrun, Burgard, Fox - &quot;Probabilistic Robotics&quot; (2005), Chapter 3</li>
<li class=""><a href="https://www.kalmanfilter.net/" target="_blank" rel="noopener noreferrer" class="">Kalman Filter Explained Visually</a></li>
</ul></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">​</a></h2>
<p>Humanoid robots rely on diverse sensor modalities:</p>
<ol>
<li class=""><strong>LIDAR</strong>: Accurate 3D mapping, long range, works in darkness (VLP-16: �3 cm precision, 100 m range)</li>
<li class=""><strong>Cameras</strong>: High-resolution color and depth (RealSense D455: 1280�720 depth, 0.6-6 m range)</li>
<li class=""><strong>IMUs</strong>: High-rate orientation tracking with drift (BMI088: 2 kHz, requires fusion)</li>
<li class=""><strong>Force/Torque Sensors</strong>: Contact detection and grip control (ATI Mini45: �290 N, 1/50 N resolution)</li>
</ol>
<p><strong>Multi-sensor fusion</strong> is essential because no single sensor provides complete, reliable information. Techniques like Kalman filtering and SLAM combine sensor data to overcome individual limitations.</p>
<p><strong>Key Takeaway</strong>: Sensor selection and fusion strategy are as important as algorithms. Physical AI systems must be designed holistically, considering sensor capabilities, noise characteristics, and fusion requirements.</p>
<p><strong>Next</strong>: In <a class="" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/assessment">Assessment</a>, test your understanding of Physical AI fundamentals through multiple choice questions, short answers, and a practical coding exercise.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<ul>
<li class=""><a href="https://pdf.directindustry.com/pdf/velodynelidar/vlp-16-datasheets/182407-676097.html" target="_blank" rel="noopener noreferrer" class="">Velodyne VLP-16 Datasheet</a> - Official LIDAR specifications</li>
<li class=""><a href="https://www.intelrealsense.com/depth-camera-d455/" target="_blank" rel="noopener noreferrer" class="">Intel RealSense D455</a> - Depth camera specifications</li>
<li class=""><a href="https://www.bosch-sensortec.com/products/motion-sensors/imus/bmi088/" target="_blank" rel="noopener noreferrer" class="">Bosch BMI088 IMU</a> - High-performance IMU datasheet</li>
<li class=""><a href="https://www.ati-ia.com/products/ft/ft_models.aspx?id=Mini45" target="_blank" rel="noopener noreferrer" class="">ATI Force/Torque Sensors</a> - F/T sensor specifications</li>
<li class=""><a href="https://docs.ros.org/en/humble/p/sensor_msgs/" target="_blank" rel="noopener noreferrer" class="">ROS 2 sensor_msgs</a> - Standard sensor message types</li>
<li class="">Thrun, S., Burgard, W., &amp; Fox, D. (2005). <em>Probabilistic Robotics</em>. MIT Press. - Kalman filtering and sensor fusion</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/KHIZRA-IQBAL/My_Hackathon_Book/tree/main/book/docs/weeks/week-01-02-physical-ai/sensor-systems.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/robotics-landscape"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Humanoid Robotics Landscape</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/assessment"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Assessment</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#sensor-data-flow-in-robot-architecture" class="table-of-contents__link toc-highlight">Sensor Data Flow in Robot Architecture</a></li><li><a href="#lidar-3d-point-cloud-generation" class="table-of-contents__link toc-highlight">LIDAR: 3D Point Cloud Generation</a><ul><li><a href="#operating-principle" class="table-of-contents__link toc-highlight">Operating Principle</a></li><li><a href="#velodyne-vlp-16-specifications" class="table-of-contents__link toc-highlight">Velodyne VLP-16 Specifications</a></li><li><a href="#when-to-use-lidar" class="table-of-contents__link toc-highlight">When to Use LIDAR</a></li><li><a href="#interactive-example-lidar-point-cloud-visualization" class="table-of-contents__link toc-highlight">Interactive Example: LIDAR Point Cloud Visualization</a></li></ul></li><li><a href="#camera-systems-rgb-depth-and-stereo-vision" class="table-of-contents__link toc-highlight">Camera Systems: RGB, Depth, and Stereo Vision</a><ul><li><a href="#types-of-cameras" class="table-of-contents__link toc-highlight">Types of Cameras</a></li><li><a href="#intel-realsense-d455-depth-camera" class="table-of-contents__link toc-highlight">Intel RealSense D455 Depth Camera</a></li><li><a href="#when-to-use-each-camera-type" class="table-of-contents__link toc-highlight">When to Use Each Camera Type</a></li><li><a href="#interactive-example-camera-image-processing" class="table-of-contents__link toc-highlight">Interactive Example: Camera Image Processing</a></li></ul></li><li><a href="#inertial-measurement-units-imus" class="table-of-contents__link toc-highlight">Inertial Measurement Units (IMUs)</a><ul><li><a href="#operating-principle-1" class="table-of-contents__link toc-highlight">Operating Principle</a></li><li><a href="#bosch-bmi088-imu-specifications" class="table-of-contents__link toc-highlight">Bosch BMI088 IMU Specifications</a></li><li><a href="#the-drift-problem" class="table-of-contents__link toc-highlight">The Drift Problem</a></li><li><a href="#when-to-use-imus" class="table-of-contents__link toc-highlight">When to Use IMUs</a></li><li><a href="#interactive-example-imu-orientation-tracking" class="table-of-contents__link toc-highlight">Interactive Example: IMU Orientation Tracking</a></li></ul></li><li><a href="#forcetorque-sensors" class="table-of-contents__link toc-highlight">Force/Torque Sensors</a><ul><li><a href="#operating-principle-2" class="table-of-contents__link toc-highlight">Operating Principle</a></li><li><a href="#ati-mini45-specifications" class="table-of-contents__link toc-highlight">ATI Mini45 Specifications</a></li><li><a href="#when-to-use-forcetorque-sensors" class="table-of-contents__link toc-highlight">When to Use Force/Torque Sensors</a></li></ul></li><li><a href="#multi-sensor-fusion" class="table-of-contents__link toc-highlight">Multi-Sensor Fusion</a><ul><li><a href="#why-fusion-is-necessary" class="table-of-contents__link toc-highlight">Why Fusion is Necessary</a></li><li><a href="#fusion-techniques" class="table-of-contents__link toc-highlight">Fusion Techniques</a></li><li><a href="#interactive-example-multi-sensor-fusion" class="table-of-contents__link toc-highlight">Interactive Example: Multi-Sensor Fusion</a></li></ul></li><li><a href="#deep-dive-kalman-filter-basics" class="table-of-contents__link toc-highlight">Deep Dive: Kalman Filter Basics</a><ul><li><a href="#1-prediction-step-motion-model" class="table-of-contents__link toc-highlight">1. Prediction Step (Motion Model)</a></li><li><a href="#2-update-step-measurement" class="table-of-contents__link toc-highlight">2. Update Step (Measurement)</a></li></ul></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learning</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/My_Hackathon_Book/">Get Started</a></li><li class="footer__item"><a class="footer__link-item" href="/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/intro">Week 1-2: Physical AI</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2 Documentation<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.nvidia.com/isaac/sim/latest/index.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">NVIDIA Isaac Sim<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://gazebosim.org/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">Gazebo Tutorials<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/My_Hackathon_Book/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/KHIZRA-IQBAL/My_Hackathon_Book" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics. Built with Docusaurus.</div></div></div></footer><button class="chatButton_ZHk5" aria-label="Toggle chatbot">💬</button><button id="ask-about-selection" style="display:none" class="selectionButton_mTV8">Ask about this</button></div>
</body>
</html>