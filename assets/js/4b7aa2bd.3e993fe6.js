"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[8530],{2344:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"weeks/week-01-02-physical-ai/sensor-systems","title":"Sensor Systems for Humanoid Robots","description":"Understanding LIDAR, cameras, IMUs, force sensors, and multi-sensor fusion","source":"@site/docs/weeks/week-01-02-physical-ai/sensor-systems.md","sourceDirName":"weeks/week-01-02-physical-ai","slug":"/weeks/week-01-02-physical-ai/sensor-systems","permalink":"/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/KHIZRA-IQBAL/My_Hackathon_Book/tree/main/book/docs/weeks/week-01-02-physical-ai/sensor-systems.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Sensor Systems for Humanoid Robots","description":"Understanding LIDAR, cameras, IMUs, force sensors, and multi-sensor fusion"},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Robotics Landscape","permalink":"/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/robotics-landscape"},"next":{"title":"Assessment","permalink":"/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/assessment"}}');var r=s(4848),t=s(8453);const o={sidebar_position:5,title:"Sensor Systems for Humanoid Robots",description:"Understanding LIDAR, cameras, IMUs, force sensors, and multi-sensor fusion"},l="Sensor Systems for Humanoid Robots",c={},a=[{value:"Introduction",id:"introduction",level:2},{value:"Sensor Data Flow in Robot Architecture",id:"sensor-data-flow-in-robot-architecture",level:2},{value:"LIDAR: 3D Point Cloud Generation",id:"lidar-3d-point-cloud-generation",level:2},{value:"Operating Principle",id:"operating-principle",level:3},{value:"Velodyne VLP-16 Specifications",id:"velodyne-vlp-16-specifications",level:3},{value:"When to Use LIDAR",id:"when-to-use-lidar",level:3},{value:"Interactive Example: LIDAR Point Cloud Visualization",id:"interactive-example-lidar-point-cloud-visualization",level:3},{value:"Camera Systems: RGB, Depth, and Stereo Vision",id:"camera-systems-rgb-depth-and-stereo-vision",level:2},{value:"Types of Cameras",id:"types-of-cameras",level:3},{value:"Intel RealSense D455 Depth Camera",id:"intel-realsense-d455-depth-camera",level:3},{value:"When to Use Each Camera Type",id:"when-to-use-each-camera-type",level:3},{value:"Interactive Example: Camera Image Processing",id:"interactive-example-camera-image-processing",level:3},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:2},{value:"Operating Principle",id:"operating-principle-1",level:3},{value:"Bosch BMI088 IMU Specifications",id:"bosch-bmi088-imu-specifications",level:3},{value:"The Drift Problem",id:"the-drift-problem",level:3},{value:"When to Use IMUs",id:"when-to-use-imus",level:3},{value:"Interactive Example: IMU Orientation Tracking",id:"interactive-example-imu-orientation-tracking",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:2},{value:"Operating Principle",id:"operating-principle-2",level:3},{value:"ATI Mini45 Specifications",id:"ati-mini45-specifications",level:3},{value:"When to Use Force/Torque Sensors",id:"when-to-use-forcetorque-sensors",level:3},{value:"Multi-Sensor Fusion",id:"multi-sensor-fusion",level:2},{value:"Why Fusion is Necessary",id:"why-fusion-is-necessary",level:3},{value:"Fusion Techniques",id:"fusion-techniques",level:3},{value:"Interactive Example: Multi-Sensor Fusion",id:"interactive-example-multi-sensor-fusion",level:3},{value:"Deep Dive: Kalman Filter Basics",id:"deep-dive-kalman-filter-basics",level:2},{value:"1. Prediction Step (Motion Model)",id:"1-prediction-step-motion-model",level:3},{value:"2. Update Step (Measurement)",id:"2-update-step-measurement",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"sensor-systems-for-humanoid-robots",children:"Sensor Systems for Humanoid Robots"})}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["Humanoid robots must perceive their environment to navigate, manipulate objects, and interact with humans. Unlike digital AI systems that receive clean, structured inputs, physical robots rely on ",(0,r.jsx)(n.strong,{children:"sensors"}),"\x14devices that convert physical quantities (light, distance, force, acceleration) into electrical signals."]}),"\n",(0,r.jsx)(n.p,{children:"This page explores four primary sensor modalities used in humanoid robotics:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LIDAR"}),": 3D point cloud generation for spatial mapping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cameras"}),": RGB, depth, and stereo vision for object recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMUs"}),": Accelerometers and gyroscopes for orientation tracking"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force/Torque Sensors"}),": Contact detection and grip control"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You will learn the operating principles, specifications, limitations, and typical use cases for each sensor type. Interactive code examples let you visualize sensor data and understand how multi-sensor fusion improves robustness."}),"\n",(0,r.jsx)(n.h2,{id:"sensor-data-flow-in-robot-architecture",children:"Sensor Data Flow in Robot Architecture"}),"\n",(0,r.jsx)(n.p,{children:"Before diving into individual sensors, consider how sensor data flows through a robot system:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph Sensors["Sensor Layer"]\r\n        L[LIDAR<br/>Point Clouds]\r\n        C[Cameras<br/>RGB/Depth]\r\n        I[IMU<br/>Accel/Gyro]\r\n        F[Force/Torque<br/>Contact]\r\n    end\r\n\r\n    subgraph Processing["Processing Layer"]\r\n        LP[Point Cloud<br/>Processing]\r\n        CV[Computer Vision<br/>Object Detection]\r\n        OR[Orientation<br/>Tracking]\r\n        CT[Contact<br/>Detection]\r\n    end\r\n\r\n    subgraph Fusion["Fusion Layer"]\r\n        KF[Kalman Filter<br/>State Estimation]\r\n        SLAM[SLAM<br/>Localization]\r\n    end\r\n\r\n    subgraph Decision["Decision Layer"]\r\n        MP[Motion Planning]\r\n        GC[Grasp Control]\r\n    end\r\n\r\n    L --\x3e LP\r\n    C --\x3e CV\r\n    I --\x3e OR\r\n    F --\x3e CT\r\n\r\n    LP --\x3e KF\r\n    CV --\x3e KF\r\n    OR --\x3e KF\r\n    CT --\x3e KF\r\n\r\n    LP --\x3e SLAM\r\n    CV --\x3e SLAM\r\n\r\n    KF --\x3e MP\r\n    SLAM --\x3e MP\r\n    KF --\x3e GC\r\n    CT --\x3e GC\r\n\r\n    style L fill:#ffebcc,stroke:#ff9900\r\n    style C fill:#ccebff,stroke:#0099ff\r\n    style I fill:#ffcceb,stroke:#ff0099\r\n    style F fill:#ccffeb,stroke:#00ff99\r\n    style KF fill:#ffe6e6,stroke:#ff0000,stroke-width:3px\r\n    style SLAM fill:#ffe6e6,stroke:#ff0000,stroke-width:3px\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Figure 1"}),": Sensor data flows from raw measurements through processing layers to fusion algorithms (Kalman Filter, SLAM) that combine multiple sensor inputs for robust state estimation. Motion planning and grasp control consume fused state estimates."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Insight"}),": No single sensor is sufficient. Robust perception requires ",(0,r.jsx)(n.strong,{children:"multi-sensor fusion"})," to overcome individual sensor limitations."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"lidar-3d-point-cloud-generation",children:"LIDAR: 3D Point Cloud Generation"}),"\n",(0,r.jsx)(n.h3,{id:"operating-principle",children:"Operating Principle"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LIDAR"})," (Light Detection and Ranging) measures distances by emitting laser pulses and timing their reflection:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Laser emits a short pulse of light"}),"\n",(0,r.jsx)(n.li,{children:"Light reflects off objects in the environment"}),"\n",(0,r.jsx)(n.li,{children:"Sensor detects the reflected light"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time-of-flight"})," calculation: distance = (speed of light \ufffd time) / 2"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["By rotating the laser or using multiple laser channels, LIDAR generates a ",(0,r.jsx)(n.strong,{children:"3D point cloud"}),"\x14a set of (x, y, z) coordinates representing surfaces in the environment."]}),"\n",(0,r.jsx)(n.h3,{id:"velodyne-vlp-16-specifications",children:"Velodyne VLP-16 Specifications"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://pdf.directindustry.com/pdf/velodynelidar/vlp-16-datasheets/182407-676097.html",children:"Velodyne VLP-16"})," is a popular LIDAR sensor used in robotics research and autonomous vehicles:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Specification"}),(0,r.jsx)(n.th,{children:"Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Channels"})}),(0,r.jsx)(n.td,{children:"16"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Range"})}),(0,r.jsx)(n.td,{children:"100 m"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Precision"})}),(0,r.jsx)(n.td,{children:"\ufffd3 cm"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Horizontal FOV"})}),(0,r.jsx)(n.td,{children:"360\ufffd"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Vertical FOV"})}),(0,r.jsx)(n.td,{children:"30\ufffd (\ufffd15\ufffd)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Horizontal Resolution"})}),(0,r.jsx)(n.td,{children:"0.2\ufffd at 10 Hz"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Vertical Resolution"})}),(0,r.jsx)(n.td,{children:"2\ufffd"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Data Rate"})}),(0,r.jsx)(n.td,{children:"~300,000 points/second"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Power"})}),(0,r.jsx)(n.td,{children:"8 W"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Weight"})}),(0,r.jsx)(n.td,{children:"830 g"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-lidar",children:"When to Use LIDAR"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Strengths:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accurate 3D spatial mapping (\ufffd3 cm precision)"}),"\n",(0,r.jsx)(n.li,{children:"Works in darkness (active sensor, provides its own illumination)"}),"\n",(0,r.jsx)(n.li,{children:"Long range (up to 100 m)"}),"\n",(0,r.jsx)(n.li,{children:"Direct distance measurements (no stereo correspondence required)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"No color information (only geometric data)"}),"\n",(0,r.jsx)(n.li,{children:"Expensive ($4,000-$8,000 for VLP-16)"}),"\n",(0,r.jsx)(n.li,{children:"Reflective or transparent surfaces cause errors"}),"\n",(0,r.jsx)(n.li,{children:"Lower resolution than cameras (thousands of points vs. millions of pixels)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Typical Use Cases:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Navigation and obstacle avoidance"}),"\n",(0,r.jsx)(n.li,{children:"3D mapping (SLAM - Simultaneous Localization and Mapping)"}),"\n",(0,r.jsx)(n.li,{children:"Terrain analysis for legged robots"}),"\n",(0,r.jsx)(n.li,{children:"Object detection and tracking"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"interactive-example-lidar-point-cloud-visualization",children:"Interactive Example: LIDAR Point Cloud Visualization"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://colab.research.google.com/github/KHIZRA-IQBAL/My_Hackathon_Book/blob/main/book/colab/week-01-02/lidar-point-cloud.ipynb",children:(0,r.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})}),"\n",(0,r.jsx)(n.p,{children:"This notebook simulates a LIDAR scan of a room with walls and obstacles, then visualizes the resulting 3D point cloud."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Sparse 3D representation (thousands of points, not millions)"}),"\n",(0,r.jsx)(n.li,{children:"Uniform sampling in angular space (not Cartesian space)"}),"\n",(0,r.jsx)(n.li,{children:'How occlusions create "shadows" in the point cloud'}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"camera-systems-rgb-depth-and-stereo-vision",children:"Camera Systems: RGB, Depth, and Stereo Vision"}),"\n",(0,r.jsx)(n.h3,{id:"types-of-cameras",children:"Types of Cameras"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB Cameras"}),": Capture color images (standard cameras)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Cameras"}),": Measure distance to each pixel"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo Cameras"}),": Use two RGB cameras to compute depth through triangulation"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"intel-realsense-d455-depth-camera",children:"Intel RealSense D455 Depth Camera"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://www.intelrealsense.com/depth-camera-d455/",children:"Intel RealSense D455"})," uses ",(0,r.jsx)(n.strong,{children:"stereo vision"})," to compute depth:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Specification"}),(0,r.jsx)(n.th,{children:"Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Depth Technology"})}),(0,r.jsx)(n.td,{children:"Stereo vision"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Range"})}),(0,r.jsx)(n.td,{children:"0.6 m to 6 m"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Depth Resolution"})}),(0,r.jsx)(n.td,{children:"1280\ufffd720 at up to 90 FPS"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Depth FOV"})}),(0,r.jsx)(n.td,{children:"86\ufffd \ufffd 57\ufffd"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"RGB Sensor"})}),(0,r.jsx)(n.td,{children:"Yes (global shutter)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"IMU"})}),(0,r.jsx)(n.td,{children:"Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Baseline"})}),(0,r.jsx)(n.td,{children:"95 mm (distance between stereo cameras)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Depth Error"})}),(0,r.jsx)(n.td,{children:"< 2% at 4 m"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Interface"})}),(0,r.jsx)(n.td,{children:"USB 3.1"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-each-camera-type",children:"When to Use Each Camera Type"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"RGB Cameras:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Object recognition (colors, textures, text)"}),"\n",(0,r.jsx)(n.li,{children:"Semantic segmentation (classify pixels: person, chair, floor)"}),"\n",(0,r.jsx)(n.li,{children:"Visual servoing (tracking targets for manipulation)"}),"\n",(0,r.jsx)(n.li,{children:"Inexpensive and high resolution"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Depth Cameras:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Obstacle avoidance (know distances to objects)"}),"\n",(0,r.jsx)(n.li,{children:"Grasp pose estimation (measure object dimensions)"}),"\n",(0,r.jsx)(n.li,{children:"3D reconstruction of scenes"}),"\n",(0,r.jsx)(n.li,{children:"Works in varied lighting (active infrared illumination)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Stereo Cameras:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Depth estimation without active illumination"}),"\n",(0,r.jsx)(n.li,{children:"Outdoor use (where structured light may fail)"}),"\n",(0,r.jsx)(n.li,{children:"Combines RGB and depth in single system"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Trade-offs:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB alone"}),": No depth information"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth alone"}),": Limited range (typically < 10 m), sensitive to lighting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo"}),": Requires texture for correspondence (fails on blank walls)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"interactive-example-camera-image-processing",children:"Interactive Example: Camera Image Processing"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://colab.research.google.com/github/KHIZRA-IQBAL/My_Hackathon_Book/blob/main/book/colab/week-01-02/camera-image-processing.ipynb",children:(0,r.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})}),"\n",(0,r.jsx)(n.p,{children:"This notebook demonstrates basic image processing: loading an image, applying edge detection, and visualizing results."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"High resolution compared to LIDAR (megapixels vs. thousands of points)"}),"\n",(0,r.jsx)(n.li,{children:"Edge detection reveals object boundaries"}),"\n",(0,r.jsx)(n.li,{children:"Color information enables semantic understanding"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,r.jsx)(n.h3,{id:"operating-principle-1",children:"Operating Principle"}),"\n",(0,r.jsxs)(n.p,{children:["An ",(0,r.jsx)(n.strong,{children:"IMU"})," (Inertial Measurement Unit) combines:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accelerometer"}),": Measures linear acceleration (including gravity)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gyroscope"}),": Measures angular velocity (rotation rate)"]}),"\n",(0,r.jsxs)(n.li,{children:["Often includes ",(0,r.jsx)(n.strong,{children:"magnetometer"})," for absolute heading (not covered here)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["By integrating accelerometer and gyroscope data, robots estimate ",(0,r.jsx)(n.strong,{children:"orientation"})," (roll, pitch, yaw) and ",(0,r.jsx)(n.strong,{children:"velocity"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"bosch-bmi088-imu-specifications",children:"Bosch BMI088 IMU Specifications"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://www.bosch-sensortec.com/products/motion-sensors/imus/bmi088/",children:"Bosch BMI088"})," is a high-performance IMU used in robotics:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Range"}),(0,r.jsx)(n.th,{children:"Resolution"}),(0,r.jsx)(n.th,{children:"Noise"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Accelerometer"})}),(0,r.jsx)(n.td,{children:"\ufffd3g to \ufffd24g"}),(0,r.jsx)(n.td,{children:"16-bit"}),(0,r.jsx)(n.td,{children:"175 \ufffdg/\x1aHz"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Gyroscope"})}),(0,r.jsx)(n.td,{children:"\ufffd125\ufffd/s to \ufffd2000\ufffd/s"}),(0,r.jsx)(n.td,{children:"16-bit"}),(0,r.jsx)(n.td,{children:"0.014\ufffd/s/\x1aHz"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Output Data Rate"})}),(0,r.jsx)(n.td,{children:"Up to 2 kHz"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Interface"})}),(0,r.jsx)(n.td,{children:"SPI, I2C"}),(0,r.jsx)(n.td,{}),(0,r.jsx)(n.td,{})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"the-drift-problem",children:"The Drift Problem"}),"\n",(0,r.jsxs)(n.p,{children:["IMUs estimate orientation by ",(0,r.jsx)(n.strong,{children:"integrating"})," angular velocity:"]}),"\n",(0,r.jsx)(n.p,{children:"\ufffd(t) = \ufffd(0) + +\ufffd(t) dt"}),"\n",(0,r.jsxs)(n.p,{children:["However, sensor noise causes ",(0,r.jsx)(n.strong,{children:"drift"}),": small errors accumulate over time. After a few minutes, estimated orientation may be off by 10-20 degrees without correction."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Fuse IMU data with other sensors (cameras, GPS, magnetometer) to correct drift."]}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-imus",children:"When to Use IMUs"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Strengths:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"High update rate (1-2 kHz)"}),"\n",(0,r.jsx)(n.li,{children:"Compact and inexpensive ($10-$100)"}),"\n",(0,r.jsx)(n.li,{children:"Essential for balance control in legged robots"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Drift over time (requires sensor fusion)"}),"\n",(0,r.jsx)(n.li,{children:"No absolute position (only orientation and acceleration)"}),"\n",(0,r.jsx)(n.li,{children:"Sensitive to vibrations and electromagnetic interference"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Typical Use Cases:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Balance control (detect tilt and adjust motors to prevent falling)"}),"\n",(0,r.jsx)(n.li,{children:"Sensor fusion for state estimation"}),"\n",(0,r.jsx)(n.li,{children:"Dead reckoning (short-term position estimation)"}),"\n",(0,r.jsx)(n.li,{children:"Detecting impacts or collisions"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"interactive-example-imu-orientation-tracking",children:"Interactive Example: IMU Orientation Tracking"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://colab.research.google.com/github/KHIZRA-IQBAL/My_Hackathon_Book/blob/main/book/colab/week-01-02/imu-orientation-tracking.ipynb",children:(0,r.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})}),"\n",(0,r.jsx)(n.p,{children:"This notebook simulates an IMU measuring rotation, integrates angular velocity to estimate orientation, and demonstrates how noise causes drift over time."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"True orientation vs. estimated orientation diverge"}),"\n",(0,r.jsx)(n.li,{children:"Drift accumulates linearly with time"}),"\n",(0,r.jsx)(n.li,{children:"Need for sensor fusion to correct errors"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,r.jsx)(n.h3,{id:"operating-principle-2",children:"Operating Principle"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Force/torque sensors"})," measure mechanical forces and moments applied to a robot's end-effector (gripper, foot, wrist). They use ",(0,r.jsx)(n.strong,{children:"strain gauges"}),"\x14resistive elements that change resistance under deformation."]}),"\n",(0,r.jsx)(n.h3,{id:"ati-mini45-specifications",children:"ATI Mini45 Specifications"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://www.ati-ia.com/products/ft/ft_models.aspx?id=Mini45",children:"ATI Mini45"})," is a compact 6-axis force/torque sensor:"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Specification"}),(0,r.jsx)(n.th,{children:"Value"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Force Range"})}),(0,r.jsx)(n.td,{children:"\ufffd290 N (z-axis), \ufffd580 N (x/y-axis)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Torque Range"})}),(0,r.jsx)(n.td,{children:"\ufffd10 Nm"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Resolution"})}),(0,r.jsx)(n.td,{children:"1/50 N (force), 1/500 Nm (torque)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Dimensions"})}),(0,r.jsx)(n.td,{children:"\ufffd45 mm \ufffd 15 mm"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Weight"})}),(0,r.jsx)(n.td,{children:"100 g"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"when-to-use-forcetorque-sensors",children:"When to Use Force/Torque Sensors"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Strengths:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Detect contact (essential for safe manipulation)"}),"\n",(0,r.jsx)(n.li,{children:"Measure grip force (avoid crushing or dropping objects)"}),"\n",(0,r.jsx)(n.li,{children:"Enable compliant control (robot yields to external forces)"}),"\n",(0,r.jsx)(n.li,{children:"Force feedback for delicate assembly tasks"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Expensive ($2,000-$5,000)"}),"\n",(0,r.jsx)(n.li,{children:"Requires careful calibration"}),"\n",(0,r.jsx)(n.li,{children:"Limited to measuring forces at sensor location (not distributed sensing)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Typical Use Cases:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Grasp control"}),": Adjust grip force based on object weight and fragility"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contact detection"}),": Know when hand touches an object"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compliant manipulation"}),": Allow robot to comply with external forces (polishing, wiping)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety"}),": Detect collisions and stop motion"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"multi-sensor-fusion",children:"Multi-Sensor Fusion"}),"\n",(0,r.jsxs)(n.p,{children:["No single sensor provides complete, reliable information. Robust robot perception requires ",(0,r.jsx)(n.strong,{children:"sensor fusion"}),"\x14combining data from multiple sensors to overcome individual limitations."]}),"\n",(0,r.jsx)(n.h3,{id:"why-fusion-is-necessary",children:"Why Fusion is Necessary"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sensor"}),(0,r.jsx)(n.th,{children:"Strengths"}),(0,r.jsx)(n.th,{children:"Weaknesses"}),(0,r.jsx)(n.th,{children:"Fusion Partner"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"LIDAR"})}),(0,r.jsx)(n.td,{children:"Accurate 3D, long range"}),(0,r.jsx)(n.td,{children:"No color, expensive"}),(0,r.jsx)(n.td,{children:"Cameras (for semantics)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"RGB Camera"})}),(0,r.jsx)(n.td,{children:"High resolution, color"}),(0,r.jsx)(n.td,{children:"No depth"}),(0,r.jsx)(n.td,{children:"Depth camera or LIDAR"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Depth Camera"})}),(0,r.jsx)(n.td,{children:"Direct depth"}),(0,r.jsx)(n.td,{children:"Limited range, indoor"}),(0,r.jsx)(n.td,{children:"RGB camera or LIDAR"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"IMU"})}),(0,r.jsx)(n.td,{children:"High rate, orientation"}),(0,r.jsx)(n.td,{children:"Drift"}),(0,r.jsx)(n.td,{children:"Cameras, GPS"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Force Sensor"})}),(0,r.jsx)(n.td,{children:"Contact detection"}),(0,r.jsx)(n.td,{children:"Local measurement"}),(0,r.jsx)(n.td,{children:"Vision (predict contact)"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"fusion-techniques",children:"Fusion Techniques"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Weighted Averaging"}),": Combine sensor readings based on reliability"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Example: Average LIDAR and depth camera distance estimates, weighting LIDAR higher"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Kalman Filtering"}),": Optimal fusion assuming Gaussian noise"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Predict robot state using motion model"}),"\n",(0,r.jsx)(n.li,{children:"Update prediction using sensor measurements"}),"\n",(0,r.jsx)(n.li,{children:"Commonly used for IMU + camera fusion"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SLAM (Simultaneous Localization and Mapping)"}),": Fuse LIDAR/camera with odometry"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Build map of environment while estimating robot position"}),"\n",(0,r.jsx)(n.li,{children:"Used in autonomous navigation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"interactive-example-multi-sensor-fusion",children:"Interactive Example: Multi-Sensor Fusion"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://colab.research.google.com/github/KHIZRA-IQBAL/My_Hackathon_Book/blob/main/book/colab/week-01-02/multi-sensor-fusion.ipynb",children:(0,r.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})}),"\n",(0,r.jsx)(n.p,{children:"This notebook simulates two noisy sensors measuring the same quantity and demonstrates how weighted averaging reduces uncertainty."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fused estimate has lower noise than either individual sensor"}),"\n",(0,r.jsx)(n.li,{children:"Weights depend on sensor reliability (lower noise \ufffd higher weight)"}),"\n",(0,r.jsx)(n.li,{children:"Fusion improves as more sensors are added"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"deep-dive-kalman-filter-basics",children:"Deep Dive: Kalman Filter Basics"}),"\n",(0,r.jsxs)(n.admonition,{title:"Deep Dive: Kalman Filter for Sensor Fusion",type:"tip",children:[(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"Kalman Filter"})," is the gold standard for fusing sensor data under Gaussian noise assumptions. It operates in two steps:"]}),(0,r.jsx)(n.h3,{id:"1-prediction-step-motion-model",children:"1. Prediction Step (Motion Model)"}),(0,r.jsx)(n.p,{children:"Predict the next state based on dynamics:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"x\x02(t|t-1) = F\ufffdx(t-1) + B\ufffdu(t)"}),"\n",(0,r.jsx)(n.li,{children:"P(t|t-1) = F\ufffdP(t-1)\ufffdF^T + Q"}),"\n"]}),(0,r.jsx)(n.p,{children:"Where:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"x\x02: State estimate (position, velocity, orientation)"}),"\n",(0,r.jsx)(n.li,{children:"F: State transition matrix (how state evolves)"}),"\n",(0,r.jsx)(n.li,{children:"B: Control input matrix"}),"\n",(0,r.jsx)(n.li,{children:"u: Control input (motor commands)"}),"\n",(0,r.jsx)(n.li,{children:"P: Covariance matrix (uncertainty)"}),"\n",(0,r.jsx)(n.li,{children:"Q: Process noise covariance"}),"\n"]}),(0,r.jsx)(n.h3,{id:"2-update-step-measurement",children:"2. Update Step (Measurement)"}),(0,r.jsx)(n.p,{children:"Correct prediction using sensor measurement:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"K = P(t|t-1)\ufffdH^T\ufffd(H\ufffdP(t|t-1)\ufffdH^T + R)^-1  (Kalman Gain)"}),"\n",(0,r.jsx)(n.li,{children:"x\x02(t|t) = x\x02(t|t-1) + K\ufffd(z - H\ufffdx\x02(t|t-1))"}),"\n",(0,r.jsx)(n.li,{children:"P(t|t) = (I - K\ufffdH)\ufffdP(t|t-1)"}),"\n"]}),(0,r.jsx)(n.p,{children:"Where:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"z: Sensor measurement"}),"\n",(0,r.jsx)(n.li,{children:"H: Measurement matrix (maps state to sensor reading)"}),"\n",(0,r.jsx)(n.li,{children:"R: Measurement noise covariance"}),"\n",(0,r.jsx)(n.li,{children:"K: Kalman gain (optimal weighting)"}),"\n"]}),(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Insight"}),": Kalman gain automatically balances prediction and measurement based on their relative uncertainties. If sensors are noisy (high R), trust the prediction more. If motion model is uncertain (high Q), trust measurements more."]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Further Reading:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Thrun, Burgard, Fox - "Probabilistic Robotics" (2005), Chapter 3'}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.kalmanfilter.net/",children:"Kalman Filter Explained Visually"})}),"\n"]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Humanoid robots rely on diverse sensor modalities:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LIDAR"}),": Accurate 3D mapping, long range, works in darkness (VLP-16: \ufffd3 cm precision, 100 m range)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cameras"}),": High-resolution color and depth (RealSense D455: 1280\ufffd720 depth, 0.6-6 m range)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMUs"}),": High-rate orientation tracking with drift (BMI088: 2 kHz, requires fusion)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force/Torque Sensors"}),": Contact detection and grip control (ATI Mini45: \ufffd290 N, 1/50 N resolution)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multi-sensor fusion"})," is essential because no single sensor provides complete, reliable information. Techniques like Kalman filtering and SLAM combine sensor data to overcome individual limitations."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Takeaway"}),": Sensor selection and fusion strategy are as important as algorithms. Physical AI systems must be designed holistically, considering sensor capabilities, noise characteristics, and fusion requirements."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": In ",(0,r.jsx)(n.a,{href:"/docs/weeks/week-01-02-physical-ai/assessment",children:"Assessment"}),", test your understanding of Physical AI fundamentals through multiple choice questions, short answers, and a practical coding exercise."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://pdf.directindustry.com/pdf/velodynelidar/vlp-16-datasheets/182407-676097.html",children:"Velodyne VLP-16 Datasheet"})," - Official LIDAR specifications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://www.intelrealsense.com/depth-camera-d455/",children:"Intel RealSense D455"})," - Depth camera specifications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://www.bosch-sensortec.com/products/motion-sensors/imus/bmi088/",children:"Bosch BMI088 IMU"})," - High-performance IMU datasheet"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://www.ati-ia.com/products/ft/ft_models.aspx?id=Mini45",children:"ATI Force/Torque Sensors"})," - F/T sensor specifications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/p/sensor_msgs/",children:"ROS 2 sensor_msgs"})," - Standard sensor message types"]}),"\n",(0,r.jsxs)(n.li,{children:["Thrun, S., Burgard, W., & Fox, D. (2005). ",(0,r.jsx)(n.em,{children:"Probabilistic Robotics"}),". MIT Press. - Kalman filtering and sensor fusion"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var i=s(6540);const r={},t=i.createContext(r);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);