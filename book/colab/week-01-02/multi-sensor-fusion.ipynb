{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Sensor Fusion\n",
        "\n",
        "**Learning Objective**: Understand why robots combine multiple sensors for better perception\n",
        "\n",
        "**Estimated Time**: 12 minutes\n",
        "\n",
        "This notebook demonstrates how fusing camera and LIDAR measurements produces more accurate and robust perception than either sensor alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install numpy==1.24.0 matplotlib==3.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_distance_measurement_camera(\n",
        "    true_distance: float,\n",
        "    num_samples: int = 100\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Simulate depth camera measurements (Intel RealSense).\n",
        "    \"\"\"\n",
        "    noise_std = 0.02 * true_distance  # 2% of distance\n",
        "    measurements = np.random.normal(true_distance, noise_std, num_samples)\n",
        "    \n",
        "    # 5% miss rate\n",
        "    miss_mask = np.random.random(num_samples) < 0.05\n",
        "    measurements[miss_mask] = np.nan\n",
        "    \n",
        "    return measurements\n",
        "\n",
        "def simulate_distance_measurement_lidar(\n",
        "    true_distance: float,\n",
        "    num_samples: int = 100\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Simulate LIDAR measurements (Velodyne VLP-16).\n",
        "    \"\"\"\n",
        "    noise_std = 0.03  # ±3cm\n",
        "    measurements = np.random.normal(true_distance, noise_std, num_samples)\n",
        "    \n",
        "    # 1% miss rate (more reliable)\n",
        "    miss_mask = np.random.random(num_samples) < 0.01\n",
        "    measurements[miss_mask] = np.nan\n",
        "    \n",
        "    return measurements\n",
        "\n",
        "def weighted_fusion(\n",
        "    camera_meas: np.ndarray,\n",
        "    lidar_meas: np.ndarray,\n",
        "    camera_weight: float = 0.4,\n",
        "    lidar_weight: float = 0.6\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Fuse camera and LIDAR using weighted average.\n",
        "    \"\"\"\n",
        "    fused = np.zeros(len(camera_meas))\n",
        "    \n",
        "    for i in range(len(camera_meas)):\n",
        "        cam_val = camera_meas[i]\n",
        "        lid_val = lidar_meas[i]\n",
        "        \n",
        "        if np.isnan(cam_val) and np.isnan(lid_val):\n",
        "            fused[i] = np.nan\n",
        "        elif np.isnan(cam_val):\n",
        "            fused[i] = lid_val\n",
        "        elif np.isnan(lid_val):\n",
        "            fused[i] = cam_val\n",
        "        else:\n",
        "            fused[i] = camera_weight * cam_val + lidar_weight * lid_val\n",
        "    \n",
        "    return fused\n",
        "\n",
        "def calculate_sensor_metrics(\n",
        "    measurements: np.ndarray,\n",
        "    true_value: float\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Calculate accuracy metrics.\n",
        "    \"\"\"\n",
        "    valid_meas = measurements[~np.isnan(measurements)]\n",
        "    \n",
        "    if len(valid_meas) == 0:\n",
        "        return {'rmse': float('inf'), 'bias': float('inf'), 'miss_rate': 1.0}\n",
        "    \n",
        "    errors = valid_meas - true_value\n",
        "    return {\n",
        "        'rmse': np.sqrt(np.mean(errors ** 2)),\n",
        "        'bias': np.mean(errors),\n",
        "        'miss_rate': np.sum(np.isnan(measurements)) / len(measurements)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario: Robot measuring distance to obstacle\n",
        "true_distance = 3.5  # meters\n",
        "num_samples = 100\n",
        "\n",
        "# Simulate sensors\n",
        "camera_measurements = simulate_distance_measurement_camera(true_distance, num_samples)\n",
        "lidar_measurements = simulate_distance_measurement_lidar(true_distance, num_samples)\n",
        "fused_measurements = weighted_fusion(camera_measurements, lidar_measurements, 0.4, 0.6)\n",
        "\n",
        "# Calculate metrics\n",
        "camera_metrics = calculate_sensor_metrics(camera_measurements, true_distance)\n",
        "lidar_metrics = calculate_sensor_metrics(lidar_measurements, true_distance)\n",
        "fused_metrics = calculate_sensor_metrics(fused_measurements, true_distance)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MULTI-SENSOR FUSION: Why Robots Use Multiple Sensors\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTrue distance: {true_distance:.2f} meters\\n\")\n",
        "print(\"Performance Comparison:\")\n",
        "print(f\"\\n  Camera (Intel RealSense D455):\")\n",
        "print(f\"    • RMSE: {camera_metrics['rmse']:.4f} m\")\n",
        "print(f\"    • Bias: {camera_metrics['bias']:.4f} m\")\n",
        "print(f\"    • Miss Rate: {camera_metrics['miss_rate']*100:.1f}%\")\n",
        "print(f\"\\n  LIDAR (Velodyne VLP-16):\")\n",
        "print(f\"    • RMSE: {lidar_metrics['rmse']:.4f} m\")\n",
        "print(f\"    • Bias: {lidar_metrics['bias']:.4f} m\")\n",
        "print(f\"    • Miss Rate: {lidar_metrics['miss_rate']*100:.1f}%\")\n",
        "print(f\"\\n  Fused Estimate (Weighted Fusion):\")\n",
        "print(f\"    • RMSE: {fused_metrics['rmse']:.4f} m\")\n",
        "print(f\"    • Bias: {fused_metrics['bias']:.4f} m\")\n",
        "print(f\"    • Miss Rate: {fused_metrics['miss_rate']*100:.1f}%\")\n",
        "\n",
        "improvement = ((camera_metrics['rmse'] - fused_metrics['rmse']) / camera_metrics['rmse']) * 100\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"KEY INSIGHT: Fusion improves accuracy by {improvement:.1f}%\")\n",
        "print(\"Each sensor has different strengths and weaknesses.\")\n",
        "print(\"Combining them produces more robust perception!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sensor fusion\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "samples = np.arange(len(camera_measurements))\n",
        "\n",
        "# Individual sensors\n",
        "ax1.scatter(samples, camera_measurements, alpha=0.5, s=30, label='Camera', color='blue', marker='o')\n",
        "ax1.scatter(samples, lidar_measurements, alpha=0.5, s=30, label='LIDAR', color='red', marker='s')\n",
        "ax1.axhline(true_distance, color='green', linestyle='--', linewidth=2, label='True Distance', alpha=0.7)\n",
        "ax1.set_ylabel('Distance (meters)', fontsize=11)\n",
        "ax1.set_title('Individual Sensor Measurements', fontsize=12, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xlim([-2, len(samples)])\n",
        "\n",
        "# Fused measurements\n",
        "ax2.scatter(samples, fused_measurements, alpha=0.6, s=30, label='Fused Estimate', color='purple', marker='^')\n",
        "ax2.axhline(true_distance, color='green', linestyle='--', linewidth=2, label='True Distance', alpha=0.7)\n",
        "\n",
        "valid_fused = fused_measurements[~np.isnan(fused_measurements)]\n",
        "if len(valid_fused) > 0:\n",
        "    std_fused = np.std(valid_fused)\n",
        "    ax2.fill_between(samples, true_distance - std_fused, true_distance + std_fused,\n",
        "                     alpha=0.2, color='purple', label='±1 Std Dev')\n",
        "\n",
        "ax2.set_xlabel('Sample Number', fontsize=11)\n",
        "ax2.set_ylabel('Distance (meters)', fontsize=11)\n",
        "ax2.set_title('Fused Sensor Estimate (More Accurate!)', fontsize=12, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xlim([-2, len(samples)])\n",
        "\n",
        "plt.suptitle('Multi-Sensor Fusion: Better Together', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expected Output\n",
        "\n",
        "You should see:\n",
        "1. **Metrics**: Fused RMSE ~10-20% lower than camera alone\n",
        "2. **Visualization**: Purple fused estimate tighter around true distance\n",
        "\n",
        "**Key Takeaway**: Sensor fusion is fundamental to Physical AI. By combining complementary sensors, robots achieve more accurate and robust perception than any single sensor can provide."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
