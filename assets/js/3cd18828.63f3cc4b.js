"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[5762],{3763:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"weeks/week-01-02-physical-ai/robotics-landscape","title":"Humanoid Robotics Landscape","description":"Exploring major humanoid robotics companies and their technical approaches","source":"@site/docs/weeks/week-01-02-physical-ai/robotics-landscape.md","sourceDirName":"weeks/week-01-02-physical-ai","slug":"/weeks/week-01-02-physical-ai/robotics-landscape","permalink":"/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/robotics-landscape","draft":false,"unlisted":false,"editUrl":"https://github.com/KHIZRA-IQBAL/My_Hackathon_Book/tree/main/book/docs/weeks/week-01-02-physical-ai/robotics-landscape.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Humanoid Robotics Landscape","description":"Exploring major humanoid robotics companies and their technical approaches"},"sidebar":"tutorialSidebar","previous":{"title":"Embodied Intelligence","permalink":"/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/embodied-intelligence"},"next":{"title":"Sensor Systems for Humanoid Robots","permalink":"/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems"}}');var o=i(4848),t=i(8453);const r={sidebar_position:4,title:"Humanoid Robotics Landscape",description:"Exploring major humanoid robotics companies and their technical approaches"},a="Humanoid Robotics Landscape",l={},d=[{value:"Industry Overview",id:"industry-overview",level:2},{value:"Boston Dynamics Atlas: Model-Based Whole-Body Control",id:"boston-dynamics-atlas-model-based-whole-body-control",level:2},{value:"Overview",id:"overview",level:3},{value:"Technical Approach: Perception-Driven Model-Based Control",id:"technical-approach-perception-driven-model-based-control",level:3},{value:"Demonstration",id:"demonstration",level:3},{value:"Target Applications",id:"target-applications",level:3},{value:"Tesla Optimus: End-to-End Neural Network Learning",id:"tesla-optimus-end-to-end-neural-network-learning",level:2},{value:"Overview",id:"overview-1",level:3},{value:"Technical Approach: End-to-End Learning",id:"technical-approach-end-to-end-learning",level:3},{value:"Capabilities (2025)",id:"capabilities-2025",level:3},{value:"Demonstration",id:"demonstration-1",level:3},{value:"Target Applications",id:"target-applications-1",level:3},{value:"Figure AI: Foundation Models for Humanoid Manipulation",id:"figure-ai-foundation-models-for-humanoid-manipulation",level:2},{value:"Overview",id:"overview-2",level:3},{value:"Technical Approach: Foundation Models + Manipulation",id:"technical-approach-foundation-models--manipulation",level:3},{value:"Demonstration",id:"demonstration-2",level:3},{value:"Current Status",id:"current-status",level:3},{value:"Target Applications",id:"target-applications-2",level:3},{value:"Sanctuary AI Phoenix: Teleoperation to Autonomy",id:"sanctuary-ai-phoenix-teleoperation-to-autonomy",level:2},{value:"Overview",id:"overview-3",level:3},{value:"Technical Approach: Hybrid Teleoperation-Learning Pipeline",id:"technical-approach-hybrid-teleoperation-learning-pipeline",level:3},{value:"Demonstration",id:"demonstration-3",level:3},{value:"Target Applications",id:"target-applications-3",level:3},{value:"Agility Robotics Digit: Bipedal Locomotion Specialist",id:"agility-robotics-digit-bipedal-locomotion-specialist",level:2},{value:"Overview",id:"overview-4",level:3},{value:"Technical Approach: Locomotion-Focused Design",id:"technical-approach-locomotion-focused-design",level:3},{value:"Demonstration",id:"demonstration-4",level:3},{value:"Production and Scaling",id:"production-and-scaling",level:3},{value:"Target Applications",id:"target-applications-4",level:3},{value:"Comparing Approaches",id:"comparing-approaches",level:2},{value:"Trade-offs Summary",id:"trade-offs-summary",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"humanoid-robotics-landscape",children:"Humanoid Robotics Landscape"})}),"\n",(0,o.jsx)(n.h2,{id:"industry-overview",children:"Industry Overview"}),"\n",(0,o.jsx)(n.p,{children:"The humanoid robotics industry has experienced explosive growth in 2024-2025, with multiple companies achieving remarkable demonstrations of bipedal locomotion, dexterous manipulation, and autonomous task execution. Unlike previous decades where humanoid robots were primarily research platforms, today's systems target commercial deployment in warehouses, factories, healthcare, and even homes."}),"\n",(0,o.jsx)(n.p,{children:"This page surveys five leading humanoid platforms, examining their technical approaches, capabilities, and target applications. You will see three distinct strategies emerge:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model-Based Control"}),": Using classical robotics techniques (kinematics, dynamics, optimization) for precise, predictable behavior"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"End-to-End Learning"}),": Training neural networks to map sensor inputs directly to motor commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hybrid Approaches"}),": Combining teleoperation, imitation learning, and autonomous control"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Understanding these approaches reveals fundamental trade-offs in humanoid robot design: generalization vs. precision, data requirements vs. engineering complexity, and adaptability vs. reliability."}),"\n",(0,o.jsx)(n.h2,{id:"boston-dynamics-atlas-model-based-whole-body-control",children:"Boston Dynamics Atlas: Model-Based Whole-Body Control"}),"\n",(0,o.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Boston Dynamics has been developing humanoid robots for over a decade. Their Atlas platform represents the state-of-the-art in model-based control, using sophisticated algorithms for balance, motion planning, and perception."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Key Milestones:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"2013: DARPA Robotics Challenge (hydraulic Atlas)"}),"\n",(0,o.jsx)(n.li,{children:"2021: Parkour demonstrations with two Atlas robots"}),"\n",(0,o.jsx)(n.li,{children:"April 2024: Retirement of hydraulic Atlas"}),"\n",(0,o.jsx)(n.li,{children:"April 2024: Introduction of electric Atlas with expanded range of motion"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"technical-approach-perception-driven-model-based-control",children:"Technical Approach: Perception-Driven Model-Based Control"}),"\n",(0,o.jsxs)(n.p,{children:["Unlike pre-programmed routines, Atlas uses ",(0,o.jsx)(n.strong,{children:"perception-driven behaviors"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Computer vision"})," detects obstacles, terrain, and objects in real-time"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Optimization-based motion planning"})," computes feasible whole-body trajectories"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feedback control"})," adjusts joint torques to track planned motions despite disturbances"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Whole-Body Control"}),": Atlas can balance using all four limbs, enabling complex maneuvers like:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Parkour (jumping, vaulting, balance beams)"}),"\n",(0,o.jsx)(n.li,{children:"Dynamic manipulation (throwing, catching)"}),"\n",(0,o.jsx)(n.li,{children:"Terrain adaptation (rough ground, slopes, stairs)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"demonstration",children:"Demonstration"}),"\n",(0,o.jsx)(n.p,{children:"Watch Atlas perform parkour maneuvers requiring real-time perception and dynamic balance:"}),"\n",(0,o.jsx)("div",{style:{position:"relative",paddingBottom:"56.25%",height:0,overflow:"hidden",maxWidth:"100%"},children:(0,o.jsx)("iframe",{style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"},width:"560",height:"315",src:"https://www.youtube.com/embed/_sBBaNYex3E",title:"Boston Dynamics Atlas Parkour",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=_sBBaNYex3E",children:"Can't see the video? Watch Boston Dynamics Atlas Parkour on YouTube"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Precision landing on narrow beams (requires cm-level foot placement accuracy)"}),"\n",(0,o.jsx)(n.li,{children:"Recovery from impacts (reactive control adjusts to unexpected forces)"}),"\n",(0,o.jsx)(n.li,{children:"Complex sequences (backflips, vaults) executed fluidly"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"target-applications",children:"Target Applications"}),"\n",(0,o.jsxs)(n.p,{children:["Boston Dynamics emphasizes ",(0,o.jsx)(n.strong,{children:"manufacturing and logistics"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Material handling in warehouses"}),"\n",(0,o.jsx)(n.li,{children:"Inspection in hazardous environments (nuclear, oil & gas)"}),"\n",(0,o.jsx)(n.li,{children:"Research platform for advanced locomotion algorithms"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Limitations"}),": High cost, complex maintenance, and specialized deployment limit widespread adoption outside industrial settings."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"tesla-optimus-end-to-end-neural-network-learning",children:"Tesla Optimus: End-to-End Neural Network Learning"}),"\n",(0,o.jsx)(n.h3,{id:"overview-1",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Tesla entered humanoid robotics in 2021 with the goal of creating a general-purpose robot for manufacturing and home use. Optimus leverages Tesla's expertise in neural networks and real-world data collection from millions of vehicles."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Key Milestones:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'October 2024: "We, Robot" event demonstrations'}),"\n",(0,o.jsx)(n.li,{children:"May 2025: Autonomous factory deployment at Fremont"}),"\n",(0,o.jsx)(n.li,{children:'December 2025: Jogging demonstration ("Just set a new PR in the lab")'}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"technical-approach-end-to-end-learning",children:"Technical Approach: End-to-End Learning"}),"\n",(0,o.jsxs)(n.p,{children:["Optimus uses a ",(0,o.jsx)(n.strong,{children:"single neural network"})," for all tasks:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision-based perception"}),": Cameras provide RGB input (no LIDAR)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Neural network"})," maps images directly to joint commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Trained on massive datasets"}),": Teleoperation, simulation, and real-world deployment data"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Key Insight"}),": Instead of hand-engineering controllers for each task, Tesla trains a general policy that learns from examples. This mirrors their approach to Full Self-Driving (FSD) in vehicles."]}),"\n",(0,o.jsx)(n.h3,{id:"capabilities-2025",children:"Capabilities (2025)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Smooth jogging with improved balance and gait control"}),"\n",(0,o.jsx)(n.li,{children:"Autonomous navigation in factories with obstacle avoidance"}),"\n",(0,o.jsx)(n.li,{children:"Stair climbing and uneven terrain locomotion"}),"\n",(0,o.jsx)(n.li,{children:"Household tasks: trash disposal, sweeping, vacuuming, paper towel handling, pot stirring, cabinet operation"}),"\n",(0,o.jsx)(n.li,{children:"Onboard vision processing (no external compute required)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"demonstration-1",children:"Demonstration"}),"\n",(0,o.jsx)(n.p,{children:"Watch Tesla Optimus perform household tasks and demonstrate improved locomotion:"}),"\n",(0,o.jsx)("div",{style:{position:"relative",paddingBottom:"56.25%",height:0,overflow:"hidden",maxWidth:"100%"},children:(0,o.jsx)("iframe",{style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"},width:"560",height:"315",src:"https://www.youtube.com/embed/cpraXaw7dyc",title:"Tesla Optimus Robot Latest Demonstration",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=cpraXaw7dyc",children:"Can't see the video? Watch Tesla Optimus latest demos on YouTube"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Natural-looking gait (smoother than earlier prototypes)"}),"\n",(0,o.jsx)(n.li,{children:"Task versatility (wide range of manipulation skills)"}),"\n",(0,o.jsx)(n.li,{children:"Real-world deployment (working in Fremont factory)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"target-applications-1",children:"Target Applications"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manufacturing"}),": Factory automation (Tesla's internal use case)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"General Purpose Home Robots"}),": Long-term vision for household assistance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Target Production"}),": 5,000 units in 2025, scaling to tens of thousands by 2026"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Trade-offs"}),": End-to-end learning requires massive datasets and may lack the precision of model-based control for safety-critical tasks. However, it generalizes better to novel situations."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"figure-ai-foundation-models-for-humanoid-manipulation",children:"Figure AI: Foundation Models for Humanoid Manipulation"}),"\n",(0,o.jsx)(n.h3,{id:"overview-2",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Figure AI, founded in 2022, focuses on humanoid robots for commercial deployment. Their landmark achievement was integrating OpenAI's multimodal foundation models to enable natural language task specification and reasoning."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Key Milestone:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"March 13, 2024: Demonstration of Figure 01 with OpenAI integration"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"technical-approach-foundation-models--manipulation",children:"Technical Approach: Foundation Models + Manipulation"}),"\n",(0,o.jsx)(n.p,{children:"Figure's approach combines:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"OpenAI-trained vision-language model"}),": Processes camera images and speech"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Verbal reasoning"}),": Robot explains what it sees and why it takes actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time conversation"}),": No teleoperation during demos\x14robot responds autonomously"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Example Interaction:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Human: "I\'m hungry"'}),"\n",(0,o.jsx)(n.li,{children:'Robot: [Identifies apple on table] "I see an apple. I will give it to you." [Hands apple to human]'}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["This demonstrates ",(0,o.jsx)(n.strong,{children:"contextual understanding"})," beyond pre-programmed responses."]}),"\n",(0,o.jsx)(n.h3,{id:"demonstration-2",children:"Demonstration"}),"\n",(0,o.jsx)(n.p,{children:"Watch Figure 01 with OpenAI integration demonstrate language-grounded manipulation:"}),"\n",(0,o.jsx)("div",{style:{position:"relative",paddingBottom:"56.25%",height:0,overflow:"hidden",maxWidth:"100%"},children:(0,o.jsx)("iframe",{style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"},width:"560",height:"315",src:"https://www.youtube.com/embed/Sq1QZB5baNw",title:"Figure 01 with OpenAI Integration",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=Sq1QZB5baNw",children:"Can't see the video? Watch Figure 01 OpenAI Integration Demo"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Natural language understanding (interprets abstract requests like "I\'m hungry")'}),"\n",(0,o.jsx)(n.li,{children:"Visual grounding (identifies objects and their affordances)"}),"\n",(0,o.jsx)(n.li,{children:"Manipulation dexterity (grasping, handing objects)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"current-status",children:"Current Status"}),"\n",(0,o.jsx)(n.p,{children:'Figure ended its collaboration with OpenAI in 2025 as large language models became "commoditized." However, the March 2024 demo remains a landmark achievement in grounding foundation models in physical interaction.'}),"\n",(0,o.jsx)(n.h3,{id:"target-applications-2",children:"Target Applications"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Warehouse logistics"}),": Order fulfillment and sorting"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manufacturing"}),": Assembly and quality inspection"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Retail"}),": Restocking shelves"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Significance"}),": Demonstrates that pre-trained AI models (GPT-4) can be adapted for embodied tasks, potentially accelerating robot learning by leveraging internet-scale knowledge."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"sanctuary-ai-phoenix-teleoperation-to-autonomy",children:"Sanctuary AI Phoenix: Teleoperation to Autonomy"}),"\n",(0,o.jsx)(n.h3,{id:"overview-3",children:"Overview"}),"\n",(0,o.jsxs)(n.p,{children:["Sanctuary AI focuses on creating humanoid robots that learn tasks through ",(0,o.jsx)(n.strong,{children:"teleoperation"}),", where human operators control the robot to demonstrate skills. Recorded data is then used to train autonomous policies."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Specifications (8th Generation Phoenix, December 2024):"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Height: 170 cm"}),"\n",(0,o.jsx)(n.li,{children:"Weight: 70 kg"}),"\n",(0,o.jsx)(n.li,{children:"Maximum payload: 25 kg"}),"\n",(0,o.jsx)(n.li,{children:"Full body mobility with human-like dexterous hands"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"technical-approach-hybrid-teleoperation-learning-pipeline",children:"Technical Approach: Hybrid Teleoperation-Learning Pipeline"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Teleoperation Phase"}),": Human operator controls robot to demonstrate tasks (product sorting, assembly, manipulation)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data Collection"}),": Record sensor inputs (vision, force) and motor commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Policy Training"}),": Train neural network to replicate demonstrated behaviors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Autonomous Execution"}),": Deployed robot performs tasks independently"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Key Capability"}),": Learn new tasks in less than 24 hours"]}),"\n",(0,o.jsx)(n.h3,{id:"demonstration-3",children:"Demonstration"}),"\n",(0,o.jsx)(n.p,{children:"Watch Sanctuary AI Phoenix demonstrate dexterous manipulation and product sorting:"}),"\n",(0,o.jsx)("div",{style:{position:"relative",paddingBottom:"56.25%",height:0,overflow:"hidden",maxWidth:"100%"},children:(0,o.jsx)("iframe",{style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"},width:"560",height:"315",src:"https://www.youtube.com/embed/ccEXaHHs8aY",title:"Sanctuary AI Phoenix Humanoid Robot",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=ccEXaHHs8aY",children:"Can't see the video? Watch Sanctuary AI Phoenix Demo"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Dexterous hand manipulation (requires precise finger control)"}),"\n",(0,o.jsx)(n.li,{children:"Task complexity (sorting diverse objects by size, color, type)"}),"\n",(0,o.jsx)(n.li,{children:"Waist-up focus (less emphasis on locomotion, more on manipulation)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"target-applications-3",children:"Target Applications"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Healthcare and eldercare"}),": Assistance with daily living tasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manufacturing"}),": Complex assembly requiring dexterity"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Retail and logistics"}),": Product handling and organization"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Advantage"}),": Teleoperation allows rapid skill acquisition without requiring massive autonomous datasets. Humans provide the intelligence; robots provide the embodiment."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"agility-robotics-digit-bipedal-locomotion-specialist",children:"Agility Robotics Digit: Bipedal Locomotion Specialist"}),"\n",(0,o.jsx)(n.h3,{id:"overview-4",children:"Overview"}),"\n",(0,o.jsxs)(n.p,{children:["Agility Robotics, spun out of Oregon State University research, focuses on ",(0,o.jsx)(n.strong,{children:"bipedal locomotion for logistics"}),". Digit is designed specifically for warehouse automation rather than general-purpose tasks."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Commercial Deployment Milestone:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"November 2025: 100,000+ totes moved for GXO Logistics in multi-year deployment near Atlanta, Georgia"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"technical-approach-locomotion-focused-design",children:"Technical Approach: Locomotion-Focused Design"}),"\n",(0,o.jsx)(n.p,{children:"Digit prioritizes:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stable bipedal walking"}),": Safe navigation in human spaces"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tote manipulation"}),": Optimized for picking and placing boxes/bins"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human-centric design"}),": Can operate in aisles designed for human workers"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Unlike general-purpose humanoids, Digit is a ",(0,o.jsx)(n.strong,{children:"specialist"})," optimized for warehouse logistics."]}),"\n",(0,o.jsx)(n.h3,{id:"demonstration-4",children:"Demonstration"}),"\n",(0,o.jsx)(n.p,{children:"Watch Digit perform warehouse automation tasks in real commercial deployment:"}),"\n",(0,o.jsx)("div",{style:{position:"relative",paddingBottom:"56.25%",height:0,overflow:"hidden",maxWidth:"100%"},children:(0,o.jsx)("iframe",{style:{position:"absolute",top:0,left:0,width:"100%",height:"100%"},width:"560",height:"315",src:"https://www.youtube.com/embed/29ECwExc-_M",title:"Agility Robotics Digit Warehouse Automation",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=29ECwExc-_M",children:"Can't see the video? Watch Digit Warehouse Automation Demo"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What to observe:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Autonomous navigation alongside AMRs (Autonomous Mobile Robots)"}),"\n",(0,o.jsx)(n.li,{children:"Reliable tote handling in real commercial deployment"}),"\n",(0,o.jsx)(n.li,{children:"Human-robot collaboration in shared workspace"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"production-and-scaling",children:"Production and Scaling"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"RoboFab factory"})," in Oregon"]}),"\n",(0,o.jsx)(n.li,{children:"Current production: Hundreds of units per year"}),"\n",(0,o.jsx)(n.li,{children:"Roadmap: 10,000+ units annually"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"target-applications-4",children:"Target Applications"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Warehouse logistics"}),": Primary focus (order fulfillment, restocking)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manufacturing"}),": Material transport"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Retail"}),": Backroom automation"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Significance"}),": Digit represents the first large-scale commercial deployment of a bipedal humanoid robot. By focusing on a narrow domain (logistics), Agility Robotics achieves reliability and cost-effectiveness."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"comparing-approaches",children:"Comparing Approaches"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:'graph TB\r\n    subgraph Learning["End-to-End Learning Approach"]\r\n        T[Tesla Optimus<br/>Neural Networks]\r\n        FIG[Figure AI<br/>Foundation Models]\r\n    end\r\n\r\n    subgraph ModelBased["Model-Based Control"]\r\n        BD[Boston Dynamics Atlas<br/>Whole-Body Control]\r\n        AG[Agility Robotics Digit<br/>Bipedal Locomotion]\r\n    end\r\n\r\n    subgraph Hybrid["Hybrid: Teleoperation + Learning"]\r\n        SA[Sanctuary AI Phoenix<br/>Teleoperation to Autonomy]\r\n    end\r\n\r\n    subgraph Applications["Target Applications"]\r\n        W[Warehouse Automation]\r\n        M[Manufacturing]\r\n        HC[Healthcare & Assistance]\r\n        GP[General Purpose Home]\r\n    end\r\n\r\n    T -.-> GP\r\n    FIG -.-> GP\r\n    BD -.-> M\r\n    AG -.-> W\r\n    SA -.-> HC\r\n\r\n    style T fill:#ffe6cc,stroke:#ff9900,stroke-width:2px\r\n    style FIG fill:#ffe6cc,stroke:#ff9900,stroke-width:2px\r\n    style BD fill:#cce6ff,stroke:#0099ff,stroke-width:2px\r\n    style AG fill:#cce6ff,stroke:#0099ff,stroke-width:2px\r\n    style SA fill:#e6ccff,stroke:#9900ff,stroke-width:2px\r\n    style W fill:#e6ffe6,stroke:#00cc00\r\n    style M fill:#e6ffe6,stroke:#00cc00\r\n    style HC fill:#e6ffe6,stroke:#00cc00\r\n    style GP fill:#e6ffe6,stroke:#00cc00\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Figure 2"}),": Humanoid robotics landscape showing three technical approaches (end-to-end learning, model-based control, hybrid teleoperation) and their target applications (warehouse automation, manufacturing, healthcare, general-purpose home use)"]}),"\n",(0,o.jsx)(n.h3,{id:"trade-offs-summary",children:"Trade-offs Summary"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Approach"}),(0,o.jsx)(n.th,{children:"Strengths"}),(0,o.jsx)(n.th,{children:"Weaknesses"}),(0,o.jsx)(n.th,{children:"Examples"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Model-Based"})}),(0,o.jsx)(n.td,{children:"Precision, predictability, safety guarantees"}),(0,o.jsx)(n.td,{children:"Engineering complexity, limited generalization"}),(0,o.jsx)(n.td,{children:"Atlas, Digit"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"End-to-End Learning"})}),(0,o.jsx)(n.td,{children:"Generalization, data-driven, handles variability"}),(0,o.jsx)(n.td,{children:"Requires massive datasets, less interpretable"}),(0,o.jsx)(n.td,{children:"Optimus"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Foundation Models"})}),(0,o.jsx)(n.td,{children:"Leverages pre-trained knowledge, language grounding"}),(0,o.jsx)(n.td,{children:"Compute-intensive, requires sim-to-real transfer"}),(0,o.jsx)(n.td,{children:"Figure AI"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Teleoperation-Learning"})}),(0,o.jsx)(n.td,{children:"Rapid skill acquisition, human intelligence"}),(0,o.jsx)(n.td,{children:"Requires operator time, scalability challenges"}),(0,o.jsx)(n.td,{children:"Phoenix"})]})]})]}),"\n",(0,o.jsx)(n.p,{children:"No single approach dominates. Choice depends on:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task requirements"}),": Precision (model-based) vs. adaptability (learning)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data availability"}),": Large datasets (end-to-end) vs. human demonstrations (teleoperation)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Deployment context"}),": Controlled environments (model-based) vs. unstructured spaces (learning)"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"The humanoid robotics landscape reveals diverse strategies for achieving general-purpose physical intelligence:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Boston Dynamics Atlas"})," exemplifies model-based control with perception-driven whole-body planning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tesla Optimus"})," demonstrates end-to-end neural network learning for task versatility"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Figure AI"})," shows how foundation models enable language-grounded manipulation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sanctuary AI Phoenix"})," uses teleoperation for rapid skill demonstration and learning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Agility Robotics Digit"})," achieves commercial success through locomotion specialization"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Each platform reflects different beliefs about the path to robust, deployable humanoid robots. The next 2-5 years will reveal which approaches scale to real-world complexity."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Next"}),": In ",(0,o.jsx)(n.a,{href:"/docs/weeks/week-01-02-physical-ai/sensor-systems",children:"Sensor Systems"}),", we examine the perception systems that enable these robots to see, feel, and navigate their environments."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://www.bostondynamics.com/",children:"Boston Dynamics"})," - Official Atlas robot information and demos"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://spectrum.ieee.org/boston-dynamics-atlas-parkour",children:"IEEE Spectrum - Atlas Parkour"})," - Technical analysis of Atlas capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://www.tesla.com/optimus",children:"Tesla Optimus"})," - Official Tesla humanoid robot page"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://www.teslarati.com/tesla-optimus-most-impressive-demonstration-video/",children:"TeslaRati - Optimus Progress"})," - Optimus demonstration analysis"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://www.figure.ai/",children:"Figure AI"})," - Official Figure humanoid robot information"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://newatlas.com/robotics/figure-01-openai-humanoid-robot-real-time-conversations/",children:"New Atlas - Figure OpenAI Integration"})," - Technical details on OpenAI integration"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://www.sanctuary.ai/",children:"Sanctuary AI"})," - Phoenix robot specifications"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://spectrum.ieee.org/sanctuary-humanoid-robot",children:"IEEE Spectrum - Sanctuary Phoenix"})," - General-purpose robot analysis"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://www.agilityrobotics.com/",children:"Agility Robotics"})," - Digit robot and commercial deployments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"https://interestingengineering.com/ai-robotics/digit-humanoid-robot-moves-100000-totes",children:"Interesting Engineering - Digit Milestone"})," - 100K totes deployment milestone"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var s=i(6540);const o={},t=s.createContext(o);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);