"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[6270],{3734:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"weeks/week-01-02-physical-ai/assessment","title":"Assessment","description":"Validate your understanding of Physical AI fundamentals","source":"@site/docs/weeks/week-01-02-physical-ai/assessment.md","sourceDirName":"weeks/week-01-02-physical-ai","slug":"/weeks/week-01-02-physical-ai/assessment","permalink":"/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/assessment","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/weeks/week-01-02-physical-ai/assessment.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Assessment","description":"Validate your understanding of Physical AI fundamentals"},"sidebar":"tutorialSidebar","previous":{"title":"Sensor Systems for Humanoid Robots","permalink":"/My_Hackathon_Book/docs/weeks/week-01-02-physical-ai/sensor-systems"},"next":{"title":"Week 3-5: ROS 2 Programming for Physical AI","permalink":"/My_Hackathon_Book/docs/weeks/week-03-05-ros2/intro"}}');var t=s(4848),r=s(8453);const o={sidebar_position:6,title:"Assessment",description:"Validate your understanding of Physical AI fundamentals"},a="Week 1-2 Assessment: Physical AI Foundations",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Part 1: Multiple Choice Questions (5 questions)",id:"part-1-multiple-choice-questions-5-questions",level:2},{value:"Question 1: Physical AI Definition",id:"question-1-physical-ai-definition",level:3},{value:"Question 2: Embodied Intelligence",id:"question-2-embodied-intelligence",level:3},{value:"Question 3: Sensor Selection",id:"question-3-sensor-selection",level:3},{value:"Question 4: Sim-to-Real Gap",id:"question-4-sim-to-real-gap",level:3},{value:"Question 5: Robotics Landscape",id:"question-5-robotics-landscape",level:3},{value:"Part 2: Short Answer Questions (3 questions)",id:"part-2-short-answer-questions-3-questions",level:2},{value:"Question 6: Explain the Sim-to-Real Gap",id:"question-6-explain-the-sim-to-real-gap",level:3},{value:"Question 7: Humanoid Form Factor Justification",id:"question-7-humanoid-form-factor-justification",level:3},{value:"Question 8: Sensor Fusion Necessity",id:"question-8-sensor-fusion-necessity",level:3},{value:"Part 3: Practical Coding Exercise (1 exercise)",id:"part-3-practical-coding-exercise-1-exercise",level:2},{value:"Question 9: Sensor Data Processing and Anomaly Detection",id:"question-9-sensor-data-processing-and-anomaly-detection",level:3},{value:"Self-Evaluation Checklist",id:"self-evaluation-checklist",level:2},{value:"Answer Key",id:"answer-key",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-1-2-assessment-physical-ai-foundations",children:"Week 1-2 Assessment: Physical AI Foundations"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This assessment validates your understanding of Physical AI fundamentals, embodied intelligence, sensor systems, and the humanoid robotics landscape. It consists of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"5 Multiple Choice Questions"})," (with inline feedback for incorrect answers)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"3 Short Answer Questions"})," (with grading rubrics)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"1 Practical Coding Exercise"})," (with expected output specification)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Complete all sections and use the provided feedback to self-evaluate. This assessment covers all six learning objectives from the chapter introduction."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-1-multiple-choice-questions-5-questions",children:"Part 1: Multiple Choice Questions (5 questions)"}),"\n",(0,t.jsx)(n.h3,{id:"question-1-physical-ai-definition",children:"Question 1: Physical AI Definition"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What is the primary characteristic that distinguishes Physical AI from digital AI systems?"})}),"\n",(0,t.jsx)(n.p,{children:"A) Physical AI uses larger neural networks\nB) Physical AI interacts with the real world through sensors and actuators\nC) Physical AI requires more training data\nD) Physical AI runs on specialized hardware"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("b",{children:"Answer"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Explanation"}),": Physical AI is defined by its interaction with the physical world through sensors (perception) and actuators (action). Unlike digital AI that operates purely in software environments, Physical AI must handle noisy sensors, real-time constraints, and physical safety considerations."]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why other options are incorrect"}),":"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A"})," L Network size is not the defining characteristic. Many digital AI systems (like GPT-4) are extremely large."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"C"})," L Data requirements depend on the task, not whether the AI is physical or digital."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"D"})," L Both physical and digital AI can use GPUs, TPUs, or specialized hardware."]}),"\n"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Related Content"}),": See ",(0,t.jsx)(n.a,{href:"/docs/weeks/week-01-02-physical-ai/what-is-physical-ai#definition",children:"What is Physical AI?"})," for the formal definition."]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"question-2-embodied-intelligence",children:"Question 2: Embodied Intelligence"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"According to the concept of embodied intelligence, why does physical form matter for AI systems?"})}),"\n",(0,t.jsx)(n.p,{children:"A) Larger robots can carry more powerful computers\nB) Intelligence emerges from body-environment interaction, not just computation\nC) Physical forms are easier to manufacture than software systems\nD) Humanoid shapes are required for all AI systems"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("b",{children:"Answer"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Explanation"}),": Embodied intelligence argues that intelligence is not purely computational\x14it emerges from the continuous interaction between a physical body, its sensors, its actuators, and the environment. The body actively shapes what problems the agent faces and what solutions are feasible."]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why other options are incorrect"}),":"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A"})," L Compute power is not the reason form matters. Cloud-connected robots can use remote computing."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"C"})," L Manufacturing ease is not related to the concept of embodied intelligence."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"D"})," L Embodied intelligence applies to all physical forms, not just humanoids."]}),"\n"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Related Content"}),": See ",(0,t.jsx)(n.a,{href:"/docs/weeks/week-01-02-physical-ai/embodied-intelligence#introduction-intelligence-beyond-computation",children:"Embodied Intelligence"})," for the full discussion."]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"question-3-sensor-selection",children:"Question 3: Sensor Selection"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"A humanoid robot needs to navigate stairs in a dimly lit environment. Which sensor would be MOST appropriate for obstacle detection?"})}),"\n",(0,t.jsx)(n.p,{children:"A) RGB camera (standard color camera)\nB) LIDAR\nC) Force/torque sensor\nD) IMU (accelerometer and gyroscope)"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("b",{children:"Answer"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Explanation"}),": LIDAR is an active sensor that emits its own laser light and measures time-of-flight, making it ideal for navigation in darkness. It provides accurate 3D spatial mapping (\ufffd3 cm precision for VLP-16) regardless of ambient lighting conditions."]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why other options are incorrect"}),":"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A"})," L RGB cameras require good lighting to function. They would struggle in dimly lit environments."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"C"})," L Force/torque sensors detect contact forces but cannot detect obstacles at a distance. They are used for grasp control, not navigation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"D"})," L IMUs measure orientation and acceleration but cannot detect external obstacles."]}),"\n"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Related Content"}),": See ",(0,t.jsx)(n.a,{href:"/docs/weeks/week-01-02-physical-ai/sensor-systems#lidar-3d-point-cloud-generation",children:"LIDAR section"})," for LIDAR capabilities."]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"question-4-sim-to-real-gap",children:"Question 4: Sim-to-Real Gap"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Which technique is commonly used to bridge the sim-to-real gap when training robots in simulation?"})}),"\n",(0,t.jsx)(n.p,{children:"A) Using higher resolution graphics in the simulator\nB) Domain randomization (varying simulation parameters during training)\nC) Training for longer periods in simulation\nD) Using more powerful computers for simulation"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("b",{children:"Answer"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Explanation"}),": Domain randomization varies simulation parameters (lighting, friction, object sizes, sensor noise) during training to force the policy to be robust to variability. This helps the robot generalize to real-world conditions that differ from simulation."]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why other options are incorrect"}),":"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A"})," L Graphics quality affects visual realism but does not address physics modeling errors or sensor noise."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"C"})," L Training duration does not fix the mismatch between simulated and real physics."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"D"})," L Compute power enables faster simulation but does not improve sim-to-real transfer."]}),"\n"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Related Content"}),": See ",(0,t.jsx)(n.a,{href:"/docs/weeks/week-01-02-physical-ai/what-is-physical-ai#bridging-the-gap",children:"Sim-to-Real Gap"})," for detailed techniques."]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"question-5-robotics-landscape",children:"Question 5: Robotics Landscape"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Which humanoid robotics company focuses on end-to-end neural network learning, using a single network to map sensor inputs directly to motor commands?"})}),"\n",(0,t.jsx)(n.p,{children:"A) Boston Dynamics (Atlas)\nB) Tesla (Optimus)\nC) Agility Robotics (Digit)\nD) Sanctuary AI (Phoenix)"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("b",{children:"Answer"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Explanation"}),": Tesla Optimus uses end-to-end learning where a single neural network maps camera inputs directly to joint commands. This approach mirrors Tesla's Full Self-Driving (FSD) strategy and aims for generalization across diverse tasks through massive datasets."]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why other options are incorrect"}),":"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A"})," L Boston Dynamics uses model-based control with optimization-based motion planning, not end-to-end neural networks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"C"})," L Agility Robotics Digit uses a hybrid approach optimized for locomotion, not pure end-to-end learning."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"D"})," L Sanctuary AI uses teleoperation to collect demonstration data, then trains policies (hybrid approach)."]}),"\n"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Related Content"}),": See ",(0,t.jsx)(n.a,{href:"/docs/weeks/week-01-02-physical-ai/robotics-landscape#tesla-optimus-end-to-end-neural-network-learning",children:"Tesla Optimus section"})," for details."]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-2-short-answer-questions-3-questions",children:"Part 2: Short Answer Questions (3 questions)"}),"\n",(0,t.jsx)(n.h3,{id:"question-6-explain-the-sim-to-real-gap",children:"Question 6: Explain the Sim-to-Real Gap"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"In 2-3 sentences, explain what the sim-to-real gap is and why it matters for robot learning."})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Grading Rubric"})," (5 points total):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Definition"})," (2 points): Clearly states that sim-to-real gap is the difference between simulated physics and real-world dynamics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Causes"})," (2 points): Mentions at least two causes (physics approximations, simplified sensor models, missing variability)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Impact"})," (1 point): Explains why it matters (policies trained in sim may fail in reality)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Strong Answer"}),":"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"The sim-to-real gap refers to the difference between simulated physics and real-world dynamics. Simulations use approximations for friction, contact, and deformation, and often lack realistic sensor noise and environmental variability. This gap matters because a robot trained purely in simulation may fail when deployed in the real world due to unmodeled effects like object slipping, unexpected lighting, or sensor noise."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Weak Answer"}),":"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"The sim-to-real gap is when simulation is different from reality. It matters because the robot might not work."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What's Missing"}),": Weak answer lacks specific causes (what makes sim different?) and does not explain transfer challenges."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"question-7-humanoid-form-factor-justification",children:"Question 7: Humanoid Form Factor Justification"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Why is the humanoid form factor important for Physical AI? Provide at least two specific reasons with examples."})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Grading Rubric"})," (5 points total):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reason 1 with example"}),' (2 points): e.g., "Human environments designed for human proportions" with example (stairs, door handles)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reason 2 with example"}),' (2 points): e.g., "Tool use" with example (using screwdrivers, brooms)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Clarity and specificity"})," (1 point): Concrete examples, not vague statements"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Strong Answer"}),":"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"The humanoid form factor is important because (1) human environments are designed for human proportions\x14staircases have 20-30cm steps matching leg length, and door handles are positioned for arm reach, making wheeled or quadruped robots impractical in homes and offices; (2) billions of tools are designed for human hands, so humanoid robots can use existing screwdrivers, brooms, and keyboards without requiring custom tool redesign. This versatility enables operation in unstructured human spaces."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Weak Answer"}),":"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"Humanoids are important because they look like humans and can do tasks."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What's Missing"}),': Weak answer provides no specific reasons or examples. "Look like humans" is superficial; "do tasks" applies to any robot.']}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"question-8-sensor-fusion-necessity",children:"Question 8: Sensor Fusion Necessity"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Compare LIDAR and camera sensors for robot navigation. Why is multi-sensor fusion necessary instead of using just one sensor type?"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Grading Rubric"})," (5 points total):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LIDAR strengths/weaknesses"})," (1.5 points): e.g., accurate 3D, long range, works in dark / no color, expensive"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Camera strengths/weaknesses"})," (1.5 points): e.g., high resolution, color, semantic understanding / no direct depth (RGB), limited range (depth cameras)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fusion justification"})," (2 points): Explains that fusion combines complementary strengths and mitigates individual weaknesses"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Strong Answer"}),":"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"LIDAR provides accurate 3D spatial mapping (\ufffd3cm for VLP-16) and works in darkness, but lacks color information and is expensive. RGB cameras offer high resolution and semantic understanding (object recognition, texture), but standard cameras provide no depth, and depth cameras have limited range (0.6-6m for RealSense D455). Multi-sensor fusion is necessary because LIDAR provides geometric structure while cameras add semantic meaning\x14together they enable both accurate navigation and object recognition, overcoming the limitations of each sensor alone."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Weak Answer"}),":"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"LIDAR is better for some things and cameras are better for other things, so using both is good."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What's Missing"}),": Weak answer does not specify what each sensor is better at or why fusion improves performance."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-3-practical-coding-exercise-1-exercise",children:"Part 3: Practical Coding Exercise (1 exercise)"}),"\n",(0,t.jsx)(n.h3,{id:"question-9-sensor-data-processing-and-anomaly-detection",children:"Question 9: Sensor Data Processing and Anomaly Detection"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": You are given simulated sensor data from a robot's LIDAR system. The sensor should measure distances to walls in a rectangular room, but occasionally produces anomalous readings due to sensor glitches."]}),"\n",(0,t.jsx)(n.p,{children:"Write a Python function that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Takes a NumPy array of distance measurements"}),"\n",(0,t.jsx)(n.li,{children:"Detects anomalies (readings significantly different from neighbors)"}),"\n",(0,t.jsx)(n.li,{children:"Returns the indices of anomalous measurements"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\ndef detect_lidar_anomalies(distances, threshold=2.0):\n    """\n    Detect anomalous LIDAR measurements.\n\n    Args:\n        distances: NumPy array of distance measurements (meters)\n        threshold: Standard deviations from local mean to consider anomalous\n\n    Returns:\n        anomaly_indices: NumPy array of indices where anomalies occur\n    """\n    # YOUR CODE HERE\n    pass\n\n# Test data: mostly consistent measurements with 2 anomalies\ntest_distances = np.array([3.0, 3.1, 2.9, 3.0, 15.0, 3.1, 2.9, 3.0, 0.5, 3.0])\n\nanomalies = detect_lidar_anomalies(test_distances, threshold=2.0)\nprint(f"Anomaly indices: {anomalies}")\nprint(f"Anomalous values: {test_distances[anomalies]}")\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Anomaly indices: [4 8]\nAnomalous values: [15.   0.5]\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Implementation Hints"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use a sliding window to compute local mean and standard deviation"}),"\n",(0,t.jsx)(n.li,{children:"Compare each measurement to its local neighborhood"}),"\n",(0,t.jsxs)(n.li,{children:["Mark measurements that deviate by more than ",(0,t.jsx)(n.code,{children:"threshold"})," standard deviations"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Grading Rubric"})," (10 points total):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Correct algorithm"})," (4 points): Detects anomalies using statistical methods (local mean, std deviation, or similar)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handles edge cases"})," (2 points): Works at array boundaries without errors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Correct output"})," (3 points): Identifies the two anomalies in test data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code quality"})," (1 point): Clear variable names, logical structure"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Solution"}),":"]}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("b",{children:"Show Solution"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\ndef detect_lidar_anomalies(distances, threshold=2.0):\n    """\n    Detect anomalous LIDAR measurements using local neighborhood comparison.\n    """\n    anomaly_indices = []\n    window_size = 3  # Compare each point to 3 neighbors on each side\n\n    for i in range(len(distances)):\n        # Define neighborhood (excluding current point)\n        start = max(0, i - window_size)\n        end = min(len(distances), i + window_size + 1)\n        neighborhood = np.concatenate([distances[start:i], distances[i+1:end]])\n\n        if len(neighborhood) == 0:\n            continue\n\n        # Compute local statistics\n        local_mean = np.mean(neighborhood)\n        local_std = np.std(neighborhood)\n\n        # Check if current measurement is anomalous\n        if local_std > 0:\n            z_score = abs(distances[i] - local_mean) / local_std\n            if z_score > threshold:\n                anomaly_indices.append(i)\n\n    return np.array(anomaly_indices)\n\n# Test\ntest_distances = np.array([3.0, 3.1, 2.9, 3.0, 15.0, 3.1, 2.9, 3.0, 0.5, 3.0])\nanomalies = detect_lidar_anomalies(test_distances, threshold=2.0)\nprint(f"Anomaly indices: {anomalies}")\nprint(f"Anomalous values: {test_distances[anomalies]}")\n'})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output"}),":"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Anomaly indices: [4 8]\nAnomalous values: [15.   0.5]\n"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Explanation"}),": The solution computes a local neighborhood for each measurement (excluding the current point), calculates the mean and standard deviation of that neighborhood, and flags measurements that deviate by more than ",(0,t.jsx)(n.code,{children:"threshold"})," standard deviations. Index 4 (15.0 meters) and index 8 (0.5 meters) are both anomalous compared to the consistent ~3.0 meter readings."]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"self-evaluation-checklist",children:"Self-Evaluation Checklist"}),"\n",(0,t.jsx)(n.p,{children:"After completing the assessment, verify:"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All 5 MCQs answered (check answers against provided explanations)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Short answer 1 includes: definition, causes, and impact of sim-to-real gap"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Short answer 2 provides 2+ specific reasons with examples for humanoid form factor"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Short answer 3 compares sensor strengths/weaknesses and justifies fusion"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Coding exercise correctly identifies anomalies in test data"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Reviewed incorrect MCQ feedback and revisited related content sections"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"answer-key",children:"Answer Key"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Congratulations on completing Week 1-2: Physical AI Foundations!"})}),"\n",(0,t.jsx)(n.p,{children:"You now understand:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The fundamental differences between Physical AI and digital AI"}),"\n",(0,t.jsx)(n.li,{children:"Why embodied intelligence and physical form matter"}),"\n",(0,t.jsx)(n.li,{children:"The humanoid robotics landscape and competing technical approaches"}),"\n",(0,t.jsx)(n.li,{children:"Key sensor systems (LIDAR, cameras, IMUs, force/torque sensors)"}),"\n",(0,t.jsx)(n.li,{children:"The importance of multi-sensor fusion"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Continue your learning"})," by exploring advanced topics:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Week 3-4: Deep learning for robot perception"}),"\n",(0,t.jsx)(n.li,{children:"Week 5-6: Motion planning and control"}),"\n",(0,t.jsx)(n.li,{children:"Week 7-8: Reinforcement learning for manipulation"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Stay curious and keep building!"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(6540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);